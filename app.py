import streamlit as st
import pandas as pd
from utils import (
    perform_descriptive_analysis, plot_distribution, plot_correlation_heatmap,
    analyze_temporal_patterns, perform_clustering_analysis, detect_outliers,
    analyze_frequent_values, generate_pdf_report, get_memory_summary,
    interpret_question, get_adaptive_suggestions
)

st.set_page_config(page_title="Agente de An√°lise de Dados", page_icon="ü§ñ", layout="wide")

# Inicializa mem√≥ria
if 'agent_memory' not in st.session_state:
    st.session_state.agent_memory = {'conclusions': [], 'insights': [], 'patterns_found': [], 'analysis_history': [], 'generated_plots': []}
if 'analysis_cache' not in st.session_state:
    st.session_state.analysis_cache = {}

st.title("ü§ñ Agente Aut√¥nomo de An√°lise de Dados")
st.markdown("---")

uploaded_file = st.sidebar.file_uploader("üìÅ Upload CSV", type=["csv"])

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.success(f"‚úÖ Dataset carregado: {df.shape[0]} linhas, {df.shape[1]} colunas")
    st.dataframe(df.head())

    user_question = st.text_input("Digite sua pergunta")
    if user_question:
        analysis_type = interpret_question(user_question, df)
        st.info(f"üîç Pergunta interpretada como: {analysis_type}")

        if analysis_type == "descriptive":
            st.dataframe(perform_descriptive_analysis(df))
        elif analysis_type == "correlation":
            st.pyplot(plot_correlation_heatmap(df))
        elif analysis_type == "outliers":
            col = st.selectbox("Coluna para outliers", df.select_dtypes(include="number").columns)
            _, msg = detect_outliers(df, col)
            st.info(msg)
        elif analysis_type == "temporal":
            time_cols = [c for c in df.columns if "time" in c.lower() or "date" in c.lower()]
            if time_cols:
                st.pyplot(analyze_temporal_patterns(df, time_cols[0]))
            else:
                st.warning("Nenhuma coluna temporal encontrada")
        elif analysis_type == "clustering":
            _, msg = perform_clustering_analysis(df)
            st.info(msg)

    st.markdown("üí° Sugest√µes de Perguntas:")
    for s in get_adaptive_suggestions(df):
        st.markdown(s)

    if st.button("üìÑ Gerar Relat√≥rio PDF"):
        pdf = generate_pdf_report(df)
        st.download_button("Download PDF", pdf, "relatorio.pdf", "application/pdf")

else:
    st.info("üëÜ Fa√ßa upload de um CSV para come√ßar.")

## Criar o arquivo app.py - Aplica√ß√£o Streamlit - Agente Aut√¥nomo de An√°lise de Dados
%%writefile app.py

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import io
import json
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
from matplotlib.backends.backend_pdf import PdfPages
import base64
import re

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Agente de An√°lise de Dados Inteligente",
    page_icon="ü§ñ",
    layout="wide"
)

# Configura√ß√£o de estilo para gr√°ficos
plt.style.use("seaborn-v0_8-darkgrid")
sns.set_palette("viridis")

# Inicializar mem√≥ria do agente no session_state
if 'agent_memory' not in st.session_state:
    st.session_state.agent_memory = {
        'conclusions': [],
        'insights': [],
        'patterns_found': [],
        'analysis_history': [],
        'generated_plots': []
    }

# Cache para evitar re-execu√ß√£o de an√°lises
if 'analysis_cache' not in st.session_state:
    st.session_state.analysis_cache = {}

# T√≠tulo principal
st.title("ü§ñ Agente Aut√¥nomo de An√°lise de Dados")
st.markdown("*Inteligente e Adaptativa com Mem√≥ria, An√°lise Contextual e Exporta√ß√£o PDF*")
st.markdown("---")

# Sidebar para upload de arquivo
st.sidebar.header("üìÅ Upload do Dataset")
uploaded_file = st.sidebar.file_uploader(
    "Escolha um arquivo CSV",
    type=['csv'],
    help="Fa√ßa upload de um arquivo CSV para an√°lise autom√°tica"
)

# Sidebar - Mem√≥ria do Agente
st.sidebar.markdown("---")
st.sidebar.header("üß† Mem√≥ria do Agente")
if st.session_state.agent_memory['conclusions']:
    st.sidebar.write(f"**Conclus√µes:** {len(st.session_state.agent_memory['conclusions'])}")
    st.sidebar.write(f"**An√°lises:** {len(set(st.session_state.agent_memory['analysis_history']))}")
    if st.sidebar.button("üìã Ver Mem√≥ria Completa"):
        st.sidebar.json(st.session_state.agent_memory)
else:
    st.sidebar.write("*Aguardando primeira an√°lise...*")

if st.sidebar.button("üóëÔ∏è Limpar Mem√≥ria e Cache"):
    st.session_state.agent_memory = {
        'conclusions': [],
        'insights': [],
        'patterns_found': [],
        'analysis_history': [],
        'generated_plots': []
    }
    st.session_state.analysis_cache = {}
    st.sidebar.success("Mem√≥ria e Cache limpos!")

# Sidebar - Configura√ß√µes Avan√ßadas
st.sidebar.markdown("---")
st.sidebar.header("‚öôÔ∏è Configura√ß√µes")
max_sample_size = st.sidebar.slider("Tamanho m√°ximo da amostra para clustering:", 1000, 20000, 5000)
contamination_rate = st.sidebar.slider("Taxa de contamina√ß√£o para outliers:", 0.01, 0.20, 0.10)

# === FUN√á√ïES PRINCIPAIS ===

def add_to_memory(analysis_type, conclusion, data_info=None, plot_data=None):
    """Adiciona conclus√£o √† mem√≥ria do agente com timestamp e metadados."""
    memory_entry = {
        'timestamp': datetime.now().isoformat(),
        'analysis_type': analysis_type,
        'conclusion': conclusion,
        'data_info': data_info or {}
    }

    st.session_state.agent_memory['conclusions'].append(memory_entry)
    st.session_state.agent_memory['analysis_history'].append(analysis_type)

    if plot_data:
        st.session_state.agent_memory['generated_plots'].append(plot_data)

def get_memory_summary():
    """Retorna resumo inteligente da mem√≥ria do agente."""
    memory = st.session_state.agent_memory
    if not memory['conclusions']:
        return "ü§ñ **Agente iniciado.** Nenhuma an√°lise realizada ainda."

    summary = f"üß† **Resumo da Mem√≥ria do Agente:**\n\n"
    summary += f"- **Total de an√°lises:** {len(memory['conclusions'])}\n"
    summary += f"- **Tipos realizados:** {', '.join(set(memory['analysis_history']))}\n"
    summary += f"- **√öltima an√°lise:** {memory['conclusions'][-1]['analysis_type']}\n"
    summary += f"- **Per√≠odo de atividade:** {len(set([c['timestamp'][:10] for c in memory['conclusions']]))} dia(s)\n\n"

    summary += "üìä **Principais Descobertas:**\n"
    for i, conclusion in enumerate(memory['conclusions'][-5:], 1):
        summary += f"{i}. **{conclusion['analysis_type'].title()}:** {conclusion['conclusion']}\n"

    return summary

def get_dataset_info(df):
    """Retorna informa√ß√µes descritivas completas do dataset."""
    info = f"""
üìä **INFORMA√á√ïES DESCRITIVAS DO DATASET**

**Dimens√µes:**
- Linhas: {df.shape[0]:,}
- Colunas: {df.shape[1]}
- Tamanho em mem√≥ria: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB

**Qualidade dos Dados:**
- Valores nulos: {df.isnull().sum().sum():,}
- Valores √∫nicos (m√©dia): {df.nunique().mean():.0f}
- Completude: {((df.shape[0] * df.shape[1] - df.isnull().sum().sum()) / (df.shape[0] * df.shape[1]) * 100):.1f}%

**Tipos de Colunas:**
- Num√©ricas: {len(df.select_dtypes(include=[np.number]).columns)}
- Categ√≥ricas: {len(df.select_dtypes(include=['object', 'category']).columns)}
- Datetime: {len(df.select_dtypes(include=['datetime']).columns)}

**Estat√≠sticas Gerais:**
- Densidade de dados: {(df.count().sum() / (df.shape[0] * df.shape[1]) * 100):.1f}%
- Variabilidade m√©dia: {df.select_dtypes(include=[np.number]).std().mean():.2f}
"""
    return info

def generate_complete_analysis_summary(df):
    """Gera resumo completo de todas as an√°lises realizadas, garantindo que n√£o haja repeti√ß√µes."""
    summary_text = f"""
ü§ñ **RESUMO COMPLETO DAS AN√ÅLISES - AGENTE AUT√îNOMO**

{get_dataset_info(df)}

üìà **AN√ÅLISES REALIZADAS:**

"""

    # Executar an√°lises se ainda n√£o foram feitas (usando cache)
    if "descriptive_analysis" not in st.session_state.analysis_cache:
        perform_descriptive_analysis(df)
    if "frequency_analysis" not in st.session_state.analysis_cache:
        analyze_frequent_values(df)
    if "correlation_analysis" not in st.session_state.analysis_cache:
        plot_correlation_heatmap(df)
    if "temporal_analysis" not in st.session_state.analysis_cache and 'Time' in df.columns:
        analyze_temporal_patterns(df, 'Time')
    if "clustering_analysis" not in st.session_state.analysis_cache and len(df.select_dtypes(include=[np.number]).columns) >= 2:
        perform_clustering_analysis(df)

    # Compilar todas as conclus√µes da mem√≥ria
    for i, conclusion_entry in enumerate(st.session_state.agent_memory['conclusions'], 1):
        analysis_name = conclusion_entry['analysis_type'].replace('_', ' ').title()
        summary_text += f"\n**{i}. {analysis_name}:**\n"
        summary_text += f"   {conclusion_entry['conclusion']}\n"
        summary_text += f"   *Realizada em: {conclusion_entry['timestamp'][:19]}*\n"

    summary_text += f"""

üéØ **INSIGHTS PRINCIPAIS:**

1. **Qualidade dos Dados:** Dataset com {df.shape[0]:,} registros e {df.shape[1]} vari√°veis, apresentando {((df.shape[0] * df.shape[1] - df.isnull().sum().sum()) / (df.shape[0] * df.shape[1]) * 100):.1f}% de completude.

2. **Estrutura:** Composto por {len(df.select_dtypes(include=[np.number]).columns)} vari√°veis num√©ricas e {len(df.select_dtypes(include=['object', 'category']).columns)} categ√≥ricas.

3. **Padr√µes Identificados:** {len(st.session_state.agent_memory['conclusions'])} an√°lises diferentes foram executadas, revelando padr√µes em distribui√ß√µes, correla√ß√µes e agrupamentos.

4. **Recomenda√ß√µes:** Com base nas an√°lises realizadas, o dataset apresenta caracter√≠sticas adequadas para modelagem preditiva e an√°lise explorat√≥ria avan√ßada.

üìä **TOTAL DE AN√ÅLISES EXECUTADAS:** {len(st.session_state.agent_memory['conclusions'])}
üïê **GERADO EM:** {datetime.now().strftime("%d/%m/%Y √†s %H:%M:%S")}

---
ü§ñ **Agente Aut√¥nomo de An√°lise de Dados - I2A2 Academy 2025**
"""

    return summary_text

def analyze_frequent_values(df, max_categories=10):
    """An√°lise de valores mais/menos frequentes."""
    cache_key = "frequency_analysis"
    if cache_key in st.session_state.analysis_cache:
        return st.session_state.analysis_cache[cache_key]

    categorical_cols = df.select_dtypes(include=['object', 'category']).columns
    discrete_cols = [col for col in df.select_dtypes(include=['int64']).columns
                    if df[col].nunique() <= 20 and col not in categorical_cols]

    results = {}
    conclusions = []

    for col in categorical_cols:
        if df[col].nunique() <= max_categories:
            value_counts = df[col].value_counts()
            results[col] = {
                'type': 'categorical',
                'most_frequent': value_counts.head(3).to_dict(),
                'least_frequent': value_counts.tail(3).to_dict(),
                'unique_count': df[col].nunique(),
                'null_count': df[col].isnull().sum()
            }

            most_freq_val = value_counts.index[0]
            most_freq_count = value_counts.iloc[0]
            conclusions.append(f"Coluna '{col}': valor mais frequente √© '{most_freq_val}' ({most_freq_count} ocorr√™ncias)")

    for col in discrete_cols:
        value_counts = df[col].value_counts()
        results[col] = {
            'type': 'discrete',
            'most_frequent': value_counts.head(3).to_dict(),
            'least_frequent': value_counts.tail(3).to_dict(),
            'unique_count': df[col].nunique(),
            'null_count': df[col].isnull().sum()
        }

        most_freq_val = value_counts.index[0]
        most_freq_count = value_counts.iloc[0]
        conclusions.append(f"Coluna '{col}': valor mais frequente √© {most_freq_val} ({most_freq_count} ocorr√™ncias)")

    if conclusions:
        conclusion_text = f"An√°lise de frequ√™ncia: {len(results)} colunas analisadas. " + "; ".join(conclusions[:3])
        add_to_memory("frequency_analysis", conclusion_text, {"analyzed_columns": len(results)})

    st.session_state.analysis_cache[cache_key] = results
    return results

def perform_descriptive_analysis(df):
    """An√°lise descritiva aprimorada com insights autom√°ticos."""
    cache_key = "descriptive_analysis"
    if cache_key in st.session_state.analysis_cache:
        return st.session_state.analysis_cache[cache_key]

    desc_stats = df.describe()

    numeric_cols = df.select_dtypes(include=[np.number]).columns
    conclusions = []

    for col in numeric_cols[:5]: # Limitar para n√£o sobrecarregar a mem√≥ria
        mean_val = df[col].mean()
        std_val = df[col].std()
        median_val = df[col].median()

        if std_val == 0: # Evitar divis√£o por zero
            skew_desc = "constante"
        elif abs(mean_val - median_val) > std_val * 0.5:
            skew_desc = "distribui√ß√£o assim√©trica"
        else:
            skew_desc = "distribui√ß√£o aproximadamente sim√©trica"

        conclusions.append(f"'{col}': {skew_desc}, m√©dia={mean_val:.2f}, mediana={median_val:.2f}")

    conclusion_text = f"An√°lise descritiva completa de {len(numeric_cols)} vari√°veis num√©ricas. " + "; ".join(conclusions[:3])
    add_to_memory("descriptive_analysis", conclusion_text, {
        "numeric_columns": len(numeric_cols),
        "total_rows": len(df),
        "total_columns": len(df.columns)
    })

    st.session_state.analysis_cache[cache_key] = desc_stats
    return desc_stats

def plot_distribution(df, column):
    """Gera histograma aprimorado com an√°lise estat√≠stica detalhada."""
    cache_key = f"distribution_analysis_{column}"
    if cache_key in st.session_state.analysis_cache:
        return st.session_state.analysis_cache[cache_key]

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    sns.histplot(df[column], kde=True, ax=ax1)
    ax1.set_title(f'Distribui√ß√£o de {column}')
    ax1.set_xlabel(column)
    ax1.set_ylabel('Frequ√™ncia')
    ax1.grid(True, alpha=0.3)

    sns.boxplot(y=df[column], ax=ax2)
    ax2.set_title(f'Box Plot - {column}')
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()

    skewness = df[column].skew()
    kurtosis = df[column].kurtosis()
    q1, q3 = df[column].quantile([0.25, 0.75])
    iqr = q3 - q1
    outliers_count = len(df[(df[column] < q1 - 1.5*iqr) | (df[column] > q3 + 1.5*iqr)])

    if abs(skewness) > 1:
        skew_desc = "altamente assim√©trica"
    elif abs(skewness) > 0.5:
        skew_desc = "moderadamente assim√©trica"
    else:
        skew_desc = "aproximadamente sim√©trica"

    conclusion = f"Distribui√ß√£o de '{column}': {skew_desc} (skew={skewness:.2f}), "
    conclusion += f"curtose={kurtosis:.2f}, {outliers_count} outliers detectados pelo m√©todo IQR"

    add_to_memory("distribution_analysis", conclusion, {
        "column": column,
        "skewness": float(skewness),
        "kurtosis": float(kurtosis),
        "outliers_iqr": int(outliers_count)
    })

    st.session_state.analysis_cache[cache_key] = (fig, conclusion)
    return fig, conclusion

def plot_correlation_heatmap(df):
    """Gera mapa de correla√ß√£o aprimorado com an√°lise de signific√¢ncia."""
    cache_key = "correlation_analysis"
    if cache_key in st.session_state.analysis_cache:
        return st.session_state.analysis_cache[cache_key]

    numeric_cols = df.select_dtypes(include=[np.number]).columns

    if len(numeric_cols) < 2:
        return None

    fig, ax = plt.subplots(figsize=(16, 12))
    corr_matrix = df[numeric_cols].corr()
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

    sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='RdBu_r',
                center=0, square=True, fmt='.2f', ax=ax,
                cbar_kws={"shrink": .8})
    ax.set_title('Matriz de Correla√ß√£o - An√°lise Completa', fontsize=16, pad=20)

    high_corr_pairs = []
    moderate_corr_pairs = []

    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_val = corr_matrix.iloc[i, j]
            col1, col2 = corr_matrix.columns[i], corr_matrix.columns[j]

            if abs(corr_val) > 0.7:
                high_corr_pairs.append((col1, col2, corr_val))
            elif abs(corr_val) > 0.5:
                moderate_corr_pairs.append((col1, col2, corr_val))

    conclusion = f"An√°lise de correla√ß√£o entre {len(numeric_cols)} vari√°veis: "
    conclusion += f"{len(high_corr_pairs)} correla√ß√µes altas (>0.7), "
    conclusion += f"{len(moderate_corr_pairs)} correla√ß√µes moderadas (0.5-0.7). "

    if high_corr_pairs:
        top_corr = max(high_corr_pairs, key=lambda x: abs(x[2]))
        conclusion += f"Maior correla√ß√£o: {top_corr[0]} ‚Üî {top_corr[1]} ({top_corr[2]:.3f})"

    add_to_memory("correlation_analysis", conclusion, {
        "high_correlations": len(high_corr_pairs),
        "moderate_correlations": len(moderate_corr_pairs),
        "variables_analyzed": len(numeric_cols)
    })

    st.session_state.analysis_cache[cache_key] = fig
    return fig

def analyze_temporal_patterns(df, time_column):
    """An√°lise gen√©rica de padr√µes temporais, adapt√°vel a qualquer dataset."""
    cache_key = f"temporal_analysis_{time_column}"
    if cache_key in st.session_state.analysis_cache:
        return st.session_state.analysis_cache[cache_key]

    if time_column not in df.columns:
        return None, f"‚ùå Coluna '{time_column}' n√£o encontrada no dataset"

    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

    # Distribui√ß√£o temporal geral
    ax1.hist(df[time_column], bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    ax1.set_title(f'Distribui√ß√£o Temporal - {time_column}')
    ax1.set_xlabel('Tempo')
    ax1.set_ylabel('Frequ√™ncia')
    ax1.grid(True, alpha=0.3)

    # Procurar coluna categ√≥rica/bin√°ria para segmenta√ß√£o
    cat_cols = [col for col in df.columns if df[col].nunique() <= 10 and col != time_column]

    if cat_cols:
        ref_col = cat_cols[0]  # usa a primeira encontrada
        for val in sorted(df[ref_col].dropna().unique()):
            subset = df[df[ref_col] == val]
            ax2.hist(subset[time_column], bins=30, alpha=0.6, label=f"{ref_col}={val} (n={len(subset)})")
        ax2.set_title(f"Padr√µes Temporais por {ref_col}")
        ax2.set_xlabel('Tempo')
        ax2.set_ylabel('Frequ√™ncia')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
    else:
        # fallback: tend√™ncia geral
        time_sorted = df.sort_values(time_column)
        ax2.plot(time_sorted[time_column], range(len(time_sorted)), alpha=0.7, color='orange')
        ax2.set_title('Tend√™ncia Temporal dos Dados')
        ax2.set_xlabel('Tempo')
        ax2.set_ylabel('Ordem dos Registros')
        ax2.grid(True, alpha=0.3)

    # Diferen√ßas entre tempos consecutivos
    time_diffs = df[time_column].diff().dropna()
    ax3.hist(time_diffs, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')
    ax3.set_title('Distribui√ß√£o de Intervalos Temporais')
    ax3.set_xlabel('Diferen√ßa de Tempo')
    ax3.set_ylabel('Frequ√™ncia')
    ax3.grid(True, alpha=0.3)

    # Boxplot do tempo
    ax4.boxplot(df[time_column])
    ax4.set_title(f'Box Plot - {time_column}')
    ax4.set_ylabel('Tempo')
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()

    # Estat√≠sticas descritivas da coluna temporal
    time_range = df[time_column].max() - df[time_column].min()
    time_mean = df[time_column].mean()
    time_std = df[time_column].std()
    time_median = df[time_column].median()
    concentration_ratio = time_std / time_mean if time_mean != 0 else 0

    if concentration_ratio < 0.3:
        temporal_pattern = "dados altamente concentrados temporalmente"
    elif concentration_ratio < 0.7:
        temporal_pattern = "dados moderadamente distribu√≠dos no tempo"
    else:
        temporal_pattern = "dados amplamente distribu√≠dos ao longo do tempo"

    if abs(time_mean - time_median) > time_std * 0.5:
        trend_desc = "com tend√™ncia assim√©trica"
    else:
        trend_desc = "com distribui√ß√£o temporal equilibrada"

    conclusion = f"An√°lise temporal de {len(df)} registros: per√≠odo de {time_range:.0f} unidades, "
    conclusion += f"{temporal_pattern} {trend_desc}. "
    conclusion += f"Tempo m√©dio: {time_mean:.0f}, mediana: {time_median:.0f}"

    add_to_memory("temporal_analysis", conclusion, {
        "time_column": time_column,
        "time_range": float(time_range),
        "time_mean": float(time_mean),
        "concentration_ratio": float(concentration_ratio),
        "records_analyzed": len(df)
    })

    st.session_state.analysis_cache[cache_key] = (fig, conclusion)
    return fig, conclusion


def perform_clustering_analysis(df, n_clusters=None, sample_size=None):
    """An√°lise de clustering aprimorada com otimiza√ß√£o autom√°tica."""
    cache_key = "clustering_analysis"
    if cache_key in st.session_state.analysis_cache:
        return st.session_state.analysis_cache[cache_key]

    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) < 2:
        return None, "‚ùå Necess√°rio pelo menos 2 colunas num√©ricas para clustering"

    if sample_size is None:
        sample_size = max_sample_size

    actual_sample_size = min(sample_size, len(df))

    # Amostragem inteligente para datasets grandes
    if len(df) > sample_size:
        if 'Class' in df.columns and df['Class'].nunique() == 2: # Se for bin√°rio e desbalanceado
            df_sample = df.groupby('Class', group_keys=False).apply(
                lambda x: x.sample(min(len(x), sample_size // df['Class'].nunique()), random_state=42)
            )
        else:
            df_sample = df.sample(n=actual_sample_size, random_state=42)
    else:
        df_sample = df

    df_numeric = df_sample[numeric_cols]

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df_numeric.fillna(df_numeric.mean()))

    if n_clusters is None:
        silhouette_scores = []
        K_range = range(2, min(11, len(df_sample)//50 + 2)) # Evitar muitos clusters para amostras pequenas

        if len(K_range) < 1: # Garantir que K_range n√£o esteja vazio
            return None, "‚ùå Amostra muito pequena para clustering significativo."

        for k in K_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(X_scaled)
            silhouette_avg = silhouette_score(X_scaled, cluster_labels)
            silhouette_scores.append(silhouette_avg)

        n_clusters = K_range[np.argmax(silhouette_scores)]
        best_silhouette = max(silhouette_scores)
    else:
        best_silhouette = None

    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_scaled)

    if best_silhouette is None:
        silhouette_avg = silhouette_score(X_scaled, cluster_labels)
    else:
        silhouette_avg = best_silhouette

    cluster_sizes = [np.sum(cluster_labels == i) for i in range(n_clusters)]

    if silhouette_avg > 0.7:
        quality_desc = "excelente separa√ß√£o"
    elif silhouette_avg > 0.5:
        quality_desc = "boa separa√ß√£o"
    elif silhouette_avg > 0.3:
        quality_desc = "separa√ß√£o moderada"
    else:
        quality_desc = "separa√ß√£o fraca"

    conclusion = f"Clustering K-means identificou {n_clusters} grupos com {quality_desc} "
    conclusion += f"(Silhouette: {silhouette_avg:.3f}). "
    conclusion += f"Maior cluster: {max(cluster_sizes)} pontos, menor: {min(cluster_sizes)} pontos. "
    conclusion += f"An√°lise baseada em amostra de {actual_sample_size} registros"

    add_to_memory("clustering_analysis", conclusion, {
        "n_clusters": n_clusters,
        "silhouette_score": float(silhouette_avg),
        "cluster_sizes": cluster_sizes,
        "sample_size": actual_sample_size,
        "quality_rating": quality_desc
    })

    st.session_state.analysis_cache[cache_key] = (None, conclusion) # Cacheia o resultado da an√°lise
    return None, conclusion  # Retornando None para figura para simplificar

def detect_outliers(df, column):
    """Detec√ß√£o de outliers aprimorada com m√∫ltiplos m√©todos."""
    cache_key = f"outlier_detection_{column}"
    if cache_key in st.session_state.analysis_cache:
        return st.session_state.analysis_cache[cache_key]

    if column not in df.columns:
        return None, f"‚ùå Coluna '{column}' n√£o encontrada"

    data = df[[column]].dropna()

    if len(data) < 10: # M√≠nimo de dados para IsolationForest
        return None, f"‚ùå Poucos dados na coluna '{column}' para detec√ß√£o de outliers."

    iso_forest = IsolationForest(contamination=contamination_rate, random_state=42)
    outliers_iso = iso_forest.fit_predict(data)

    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    outliers_iqr = (data[column] < (Q1 - 1.5 * IQR)) | (data[column] > (Q3 + 1.5 * IQR))

    z_scores = np.abs((data[column] - data[column].mean()) / data[column].std())
    outliers_zscore = z_scores > 3

    outlier_count_iso = sum(1 for x in outliers_iso if x == -1)
    outlier_count_iqr = sum(outliers_iqr)
    outlier_count_zscore = sum(outliers_zscore)

    total_points = len(data)

    consensus_outliers = 0
    for i in range(len(data)):
        methods_agree = 0
        if outliers_iso[i] == -1:
            methods_agree += 1
        if outliers_iqr.iloc[i]:
            methods_agree += 1
        if outliers_zscore.iloc[i]:
            methods_agree += 1

        if methods_agree >= 2:
            consensus_outliers += 1

    conclusion = f"An√°lise de outliers em '{column}': "
    conclusion += f"Isolation Forest: {outlier_count_iso} ({outlier_count_iso/total_points*100:.1f}%), "
    conclusion += f"IQR: {outlier_count_iqr} ({outlier_count_iqr/total_points*100:.1f}%), "
    conclusion += f"Z-score: {outlier_count_zscore} ({outlier_count_zscore/total_points*100:.1f}%). "
    conclusion += f"Consenso (‚â•2 m√©todos): {consensus_outliers} outliers"

    add_to_memory("outlier_detection", conclusion, {
        "column": column,
        "isolation_forest": outlier_count_iso,
        "iqr_method": outlier_count_iqr,
        "zscore_method": outlier_count_zscore,
        "consensus_outliers": consensus_outliers,
        "total_points": total_points
    })

    st.session_state.analysis_cache[cache_key] = (None, conclusion)
    return None, conclusion

def generate_pdf_report(df):
    """Gera relat√≥rio PDF com todas as an√°lises e informa√ß√µes do dataset."""
    pdf_buffer = io.BytesIO()

    with PdfPages(pdf_buffer) as pdf:
        # P√°gina 1: Capa e informa√ß√µes do dataset
        fig = plt.figure(figsize=(8.27, 11.69), dpi=100) # A4 size in inches
        ax = fig.add_subplot(111)

        ax.text(0.5, 0.95, 'ü§ñ RELAT√ìRIO COMPLETO DE AN√ÅLISE DE DADOS',
                ha='center', va='top', fontsize=16, fontweight='bold')
        ax.text(0.5, 0.90, 'Agente Aut√¥nomo de An√°lise',
                ha='center', va='top', fontsize=12)
        ax.text(0.5, 0.87, f'Gerado em: {datetime.now().strftime("%d/%m/%Y %H:%M")}',
                ha='center', va='top', fontsize=10)

        # Informa√ß√µes do dataset
        dataset_info = get_dataset_info(df)
        ax.text(0.05, 0.80, dataset_info, ha='left', va='top', fontsize=8,
                wrap=True, bbox=dict(boxstyle="round,pad=0.3", facecolor="#e0f7fa", edgecolor="#00bcd4"))

        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)

        # P√°gina 2 em diante: Resumo das an√°lises e gr√°ficos
        analysis_summary_full = generate_complete_analysis_summary(df)

        # Dividir o texto do resumo em chunks para caber nas p√°ginas
        chunks = []
        current_chunk = ""
        for line in analysis_summary_full.split('\n'):
            if len(current_chunk) + len(line) < 1800: # Ajustar este valor conforme necess√°rio
                current_chunk += line + '\n'
            else:
                chunks.append(current_chunk)
                current_chunk = line + '\n'
        if current_chunk:
            chunks.append(current_chunk)

        for i, chunk in enumerate(chunks):
            fig = plt.figure(figsize=(8.27, 11.69), dpi=100)
            ax = fig.add_subplot(111)
            ax.text(0.05, 0.95, f'Resumo das An√°lises (P√°gina {i+1})', ha='left', va='top', fontsize=14, fontweight='bold')
            ax.text(0.05, 0.90, chunk, ha='left', va='top', fontsize=8, wrap=True)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
            pdf.savefig(fig, bbox_inches='tight')
            plt.close(fig)

        # Adicionar gr√°ficos gerados
        for plot_data in st.session_state.agent_memory['generated_plots']:
            fig = plot_data['figure']
            pdf.savefig(fig, bbox_inches='tight')
            plt.close(fig)

    pdf_buffer.seek(0)
    return pdf_buffer.getvalue()

def interpret_question(question, df):
    """
    Interpreta√ß√£o sem√¢ntica avan√ßada de perguntas do usu√°rio.
    Prioriza palavras-chave espec√≠ficas antes de analisar verbos gen√©ricos.
    """
    question_lower = question.lower()

    # Mapeamento de palavras-chave
    keywords_map = {
        'correlation': [r'correla√ß(√£o|√µes)', r'rela√ß(√£o|√µes)', 'correlacionar', 'associa√ß√£o', 'depend√™ncia'],
        'temporal': ['tempo', 'temporal', 'tend√™ncia', 'padr√£o temporal', 's√©rie', 'cronol√≥gico', 'data'],
        'clustering': ['cluster', 'agrupamento', 'grupo', 'segmenta√ß√£o', 'parti√ß√£o'],
        'outliers': ['outlier', 'outliers', 'anomalia', 'an√¥malo', 'at√≠pico', 'discrepante', 'aberrante'],
        'frequency': ['frequente', 'frequentes', 'comum', 'raro', 'valores', 'contagem', 'ocorr√™ncia'],
        'memory': ['mem√≥ria', 'conclus√£o', 'hist√≥rico', 'resumo', 'descobertas', 'insights'],
        'export': ['exportar', 'pdf', 'relat√≥rio', 'salvar', 'download'],
        'descriptive': ['estat√≠stica', 'estat√≠sticas', 'resumo', 'descritiva', 'm√©dia', 'mediana', 'desvio'],
        'distribution': ['distribui√ß√£o', 'histograma', 'frequ√™ncia', 'densidade', 'spread']
    }

    # Verificar palavras-chave espec√≠ficas
    for analysis_type, keywords in keywords_map.items():
        for keyword in keywords:
            if re.search(r'\b' + keyword + r'\b', question_lower): # Usar regex para palavras inteiras
                return analysis_type

    # An√°lise de verbos apenas se n√£o encontrou palavras-chave espec√≠ficas
    if any(verb in question_lower for verb in ['mostre', 'exiba', 'visualize', 'plote', 'fa√ßa']):
        # An√°lise mais espec√≠fica para verbos
        if any(word in question_lower for word in ['tempo', 'temporal', 'data']):
            return 'temporal'
        elif any(word in question_lower for word in ['correla√ß√£o', 'correla√ß√µes', 'rela√ß√£o']):
            return 'correlation'
        elif any(word in question_lower for word in ['cluster', 'agrupamento']):
            return 'clustering'
        elif any(word in question_lower for word in ['outlier', 'anomalia']):
            return 'outliers'
        elif any(word in question_lower for word in ['frequente', 'frequentes', 'contagem']):
            return 'frequency'
        elif any(word in question_lower for word in ['estat√≠stica', 'descritiva', 'resumo']):
            return 'descriptive'
        else:
            return 'distribution'  # Default para verbos gen√©ricos

    return 'general'

def get_adaptive_suggestions(df):
    """Gera sugest√µes de an√°lise adaptativas baseadas na estrutura do dataset."""
    suggestions = []

    # 1. Colunas Num√©ricas
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        suggestions.append('‚Ä¢ *"Mostre estat√≠sticas descritivas"* - Resumo das vari√°veis num√©ricas')
        if len(numeric_cols) > 1:
            suggestions.append('‚Ä¢ *"Mostre correla√ß√µes entre vari√°veis"* - Rela√ß√£o entre colunas num√©ricas')
        suggestions.append('‚Ä¢ *"Detecte outliers com m√∫ltiplos m√©todos"* - Encontra valores an√¥malos')
        suggestions.append('‚Ä¢ *"Qual a distribui√ß√£o da coluna [Nome da Coluna]?"* - Ex: Qual a distribui√ß√£o da coluna Amount?')

    # 2. Colunas Categ√≥ricas/Discretas
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns
    discrete_int_cols = [col for col in df.select_dtypes(include=['int64']).columns if df[col].nunique() <= 20]
    if len(categorical_cols) > 0 or len(discrete_int_cols) > 0:
        suggestions.append('‚Ä¢ *"Mostre valores mais frequentes"* - An√°lise de distribui√ß√£o de categorias/discretas')

    # 3. Colunas Temporais
    time_cols = [col for col in df.columns if 'time' in col.lower() or 'date' in col.lower()]
    if len(time_cols) > 0:
        suggestions.append('‚Ä¢ *"Mostre padr√µes temporais nos dados"* - An√°lise de tend√™ncias ao longo do tempo')

    # 4. Colunas Bin√°rias Desbalanceadas (ex: 'Class')
    for col in df.columns:
        if df[col].nunique() == 2:
            counts = df[col].value_counts(normalize=True)
            if counts.min() < 0.10: # Se a menor classe for menos de 10%
                suggestions.append(f'‚Ä¢ *"Analise o balanceamento da coluna {col}"* - Verifique a propor√ß√£o das classes')

    # 5. Clustering (se houver dados num√©ricos suficientes)
    if len(numeric_cols) >= 2:
        suggestions.append('‚Ä¢ *"Fa√ßa clustering autom√°tico dos dados"* - Identifica grupos naturais nos dados')

    # 6. Mem√≥ria do Agente
    suggestions.append('‚Ä¢ *"Qual sua mem√≥ria de an√°lises?"* - Consulta hist√≥rico de descobertas')

    return suggestions[:6] # Limitar a 6 sugest√µes

# === INTERFACE PRINCIPAL ===

if uploaded_file is not None:
    try:
        with st.spinner('Carregando e analisando dataset...'):
            df = pd.read_csv(uploaded_file)

        st.success(f"‚úÖ Dataset carregado com sucesso! {df.shape[0]:,} linhas e {df.shape[1]} colunas.")

        # An√°lise autom√°tica inicial (se ainda n√£o foi feita)
        if len(st.session_state.agent_memory['conclusions']) == 0:
            with st.spinner('Realizando an√°lise inicial autom√°tica...'):
                perform_descriptive_analysis(df)
                analyze_frequent_values(df)

        # Preview dos dados
        st.subheader("üìã Preview do Dataset")
        st.dataframe(df.head(10), use_container_width=True)

        # Informa√ß√µes do Dataset
        st.subheader("üìä Informa√ß√µes do Dataset")
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("üìè Linhas", f"{df.shape[0]:,}")
            st.metric("üìä Colunas", df.shape[1])

        with col2:
            st.metric("‚ùå Valores Nulos", f"{df.isnull().sum().sum():,}")
            st.metric("üíæ Tamanho", f"{df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")

        with col3:
            st.metric("üß† An√°lises na Mem√≥ria", len(st.session_state.agent_memory['conclusions']))
            st.metric("üî¢ Colunas Num√©ricas", len(df.select_dtypes(include=[np.number]).columns))

        with col4:
            st.metric("üìù Colunas Categ√≥ricas", len(df.select_dtypes(include=['object', 'category']).columns))
            st.metric("üéØ Valores √önicos (m√©dia)", f"{df.nunique().mean():.0f}")

        # Se√ß√£o de perguntas
        st.markdown("---")
        st.subheader("ü§î Fa√ßa uma Pergunta Inteligente sobre os Dados")

        user_question = st.text_input(
            "Digite sua pergunta:",
            placeholder="Ex: Mostre correla√ß√µes entre vari√°veis | Fa√ßa clustering | Detecte outliers"
        )

        # Sugest√µes adaptativas como texto simples
        st.markdown("""
        **üí° Sugest√µes de Perguntas (Adaptativas ao seu Dataset):**
        """)
        adaptive_suggestions = get_adaptive_suggestions(df)
        for suggestion in adaptive_suggestions:
            st.markdown(suggestion)

        # Processamento da pergunta
        if user_question:
            analysis_type = interpret_question(user_question, df)

            # DEBUG: Mostrar qual tipo foi identificado
            st.info(f"üîç **Pergunta interpretada como:** {analysis_type.replace('_', ' ').title()}")

            if analysis_type == 'distribution':
                st.subheader("üìä An√°lise de Distribui√ß√£o Avan√ßada")
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

                if numeric_cols:
                    col1, col2 = st.columns([1, 1])
                    with col1:
                        selected_col = st.selectbox("Selecione a coluna:", numeric_cols)
                    with col2:
                        st.write(f"**Estat√≠sticas r√°pidas de {selected_col}:**")
                        st.write(f"M√©dia: {df[selected_col].mean():.2f}")
                        st.write(f"Mediana: {df[selected_col].median():.2f}")
                        st.write(f"Desvio padr√£o: {df[selected_col].std():.2f}")

                    if st.button("üìà Gerar An√°lise Completa de Distribui√ß√£o"):
                        with st.spinner('Gerando an√°lise de distribui√ß√£o...'):
                            fig = plot_distribution(df, selected_col)
                            st.pyplot(fig)

                            st.write("**üìä Estat√≠sticas Detalhadas:**")
                            st.dataframe(df[selected_col].describe())
                else:
                    st.warning("‚ùå Nenhuma coluna num√©rica encontrada para an√°lise de distribui√ß√£o.")

            elif analysis_type == 'correlation':
                st.subheader("üîó An√°lise de Correla√ß√£o Avan√ßada")
                if st.button("üéØ Gerar Mapa de Correla√ß√£o Completo"):
                    with st.spinner('Calculando correla√ß√µes...'):
                        fig = plot_correlation_heatmap(df)
                        if fig:
                            st.pyplot(fig)
                        else:
                            st.error("‚ùå Necess√°rio pelo menos 2 colunas num√©ricas para an√°lise de correla√ß√£o.")

            elif analysis_type == 'outliers':
                st.subheader("üéØ Detec√ß√£o Avan√ßada de Outliers")
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

                if numeric_cols:
                    selected_col = st.selectbox("Selecione a coluna:", numeric_cols)

                    if st.button("üîç Detectar Outliers (M√∫ltiplos M√©todos)"):
                        with st.spinner('Executando detec√ß√£o de outliers...'):
                            fig, message = detect_outliers(df, selected_col)
                            st.info(message)
                else:
                    st.warning("‚ùå Nenhuma coluna num√©rica encontrada para detec√ß√£o de outliers.")

            elif analysis_type == 'temporal':
                st.subheader("‚è∞ An√°lise Avan√ßada de Padr√µes Temporais")
                time_cols = [col for col in df.columns if 'time' in col.lower() or 'date' in col.lower()]

                if time_cols:
                    selected_time_col = st.selectbox("Selecione a coluna temporal:", time_cols)

                    if st.button("üìà Analisar Padr√µes Temporais Completos"):
                        with st.spinner('Analisando padr√µes temporais...'):
                            fig, message = analyze_temporal_patterns(df, selected_time_col)
                            if fig:
                                st.pyplot(fig)
                            st.info(message)
                else:
                    st.warning("‚ùå Nenhuma coluna temporal encontrada no dataset.")

            elif analysis_type == 'clustering':
                st.subheader("üéØ An√°lise Avan√ßada de Clustering")

                if st.button("üéØ Executar Clustering Inteligente"):
                    with st.spinner('Executando an√°lise de clustering...'):
                        fig, message = perform_clustering_analysis(df)
                        st.info(message)

            elif analysis_type == 'frequency':
                st.subheader("üìä An√°lise de Valores Frequentes")
                if st.button("üîç Analisar Valores Mais/Menos Frequentes"):
                    with st.spinner('Analisando frequ√™ncias...'):
                        freq_results = analyze_frequent_values(df)

                        if freq_results:
                            for col, data in freq_results.items():
                                st.write(f"**Coluna: {col}** ({data['type']})")

                                col1, col2 = st.columns(2)
                                with col1:
                                    st.write("*Mais frequentes:*")
                                    for val, count in data['most_frequent'].items():
                                        st.write(f"‚Ä¢ {val}: {count}")

                                with col2:
                                    st.write("*Menos frequentes:*")
                                    for val, count in data['least_frequent'].items():
                                        st.write(f"‚Ä¢ {val}: {count}")

                                st.write(f"Valores √∫nicos: {data['unique_count']}, Nulos: {data['null_count']}")
                                st.markdown("---")
                        else:
                            st.info("Nenhuma coluna categ√≥rica ou discreta encontrada para an√°lise de frequ√™ncia.")

            elif analysis_type == 'memory':
                st.subheader("üß† Mem√≥ria Completa do Agente")
                memory_summary = get_memory_summary()
                st.markdown(memory_summary)

                if st.session_state.agent_memory['conclusions']:
                    st.write("**üìö Hist√≥rico Detalhado de An√°lises:**")
                    for i, entry in enumerate(st.session_state.agent_memory['conclusions'], 1):
                        with st.expander(f"An√°lise {i}: {entry['analysis_type'].replace('_', ' ').title()}"):
                            st.write(f"**üîç Conclus√£o:** {entry['conclusion']}")
                            st.write(f"**‚è∞ Timestamp:** {entry['timestamp']}")
                            if entry['data_info']:
                                st.write("**üìä Dados da An√°lise:**")
                                st.json(entry['data_info'])

            elif analysis_type == 'descriptive':
                st.subheader("üìà An√°lise Descritiva Completa")
                if st.button("üìä Gerar Estat√≠sticas Descritivas Avan√ßadas"):
                    with st.spinner('Gerando an√°lise descritiva...'):
                        desc_stats = perform_descriptive_analysis(df)
                        st.dataframe(desc_stats, use_container_width=True)

                        st.write("**üìã Informa√ß√µes Detalhadas do Dataset:**")
                        buffer = io.StringIO()
                        df.info(buf=buffer)
                        info_str = buffer.getvalue()
                        st.text(info_str)

        # Painel de an√°lises r√°pidas
        st.markdown("---")
        st.subheader("‚ö° Painel de An√°lises R√°pidas")

        col_buttons, col_results = st.columns([1, 2])

        with col_buttons:
            st.write("**Selecione uma an√°lise:**")

            if st.button("üìä Estat√≠sticas Descritivas", help="An√°lise descritiva completa", use_container_width=True):
                st.session_state.quick_analysis = "descriptive"

            if st.button("üîó Mapa de Correla√ß√£o", help="Correla√ß√µes entre vari√°veis", use_container_width=True):
                st.session_state.quick_analysis = "correlation"

            if st.button("‚è∞ Padr√µes Temporais", help="An√°lise temporal (se dispon√≠vel)", use_container_width=True):
                st.session_state.quick_analysis = "temporal"

            if st.button("üéØ Clustering Autom√°tico", help="Agrupamento inteligente", use_container_width=True):
                st.session_state.quick_analysis = "clustering"

            if st.button("üìä Valores Frequentes", help="An√°lise de frequ√™ncias", use_container_width=True):
                st.session_state.quick_analysis = "frequency"

            if st.button("üß† Mem√≥ria do Agente", help="Hist√≥rico de an√°lises", use_container_width=True):
                st.session_state.quick_analysis = "memory"

        with col_results:
            if 'quick_analysis' in st.session_state:
                analysis_type = st.session_state.quick_analysis

                if analysis_type == "descriptive":
                    with st.spinner('Calculando estat√≠sticas...'):
                        desc_stats = perform_descriptive_analysis(df)
                        st.dataframe(desc_stats, use_container_width=True)

                elif analysis_type == "correlation":
                    with st.spinner('Calculando correla√ß√µes...'):
                        fig = plot_correlation_heatmap(df)
                        if fig:
                            st.pyplot(fig)
                        else:
                            st.error("Necess√°rio pelo menos 2 colunas num√©ricas")

                elif analysis_type == "temporal":
                    time_cols = [col for col in df.columns if 'time' in col.lower() or 'date' in col.lower()]
                    if time_cols:
                        with st.spinner('Analisando padr√µes temporais...'):
                            fig, message = analyze_temporal_patterns(df, time_cols[0]) # Usa a primeira coluna temporal encontrada
                            if fig:
                                st.pyplot(fig)
                            st.info(message)
                    else:
                        st.warning("Coluna temporal n√£o encontrada")

                elif analysis_type == "clustering":
                    with st.spinner('Executando clustering...'):
                        fig, message = perform_clustering_analysis(df)
                        st.info(message)

                elif analysis_type == "frequency":
                    with st.spinner('Analisando frequ√™ncias...'):
                        freq_results = analyze_frequent_values(df)
                        if freq_results:
                            for col, data in list(freq_results.items())[:3]:
                                st.write(f"**{col}:** Mais frequente = {list(data['most_frequent'].keys())[0]}")
                        else:
                            st.info("Nenhuma coluna categ√≥rica encontrada")

                elif analysis_type == "memory":
                    memory_summary = get_memory_summary()
                    st.markdown(memory_summary)
            else:
                st.info("üëà Selecione uma an√°lise r√°pida ao lado para ver os resultados aqui.")

        # Central de Intelig√™ncia do Agente
        st.markdown("---")
        st.subheader("ü§ñ Central de Intelig√™ncia do Agente")

        col1, col2 = st.columns([1, 1])

        with col1:
            if st.button("üìä Executar Resumo Das An√°lises", help="Gera resumo completo de todas as an√°lises", type="primary", use_container_width=True):
                with st.spinner('Gerando resumo completo das an√°lises...'):
                    complete_summary = generate_complete_analysis_summary(df)
                    st.markdown(complete_summary)

        with col2:
            if st.button("üìÑ Gerar Relat√≥rio PDF Completo", help="Exporta relat√≥rio com dataset e an√°lises", type="secondary", use_container_width=True):
                if st.session_state.agent_memory['conclusions']:
                    with st.spinner('Gerando relat√≥rio PDF completo...'):
                        try:
                            pdf_content = generate_pdf_report(df)

                            b64_pdf = base64.b64encode(pdf_content).decode()
                            href = f'<a href="data:application/pdf;base64,{b64_pdf}" download="relatorio_completo_analise_dados.pdf">üì• Clique aqui para baixar o relat√≥rio PDF</a>'
                            st.markdown(href, unsafe_allow_html=True)

                            st.success("‚úÖ Relat√≥rio PDF gerado com sucesso!")
                        except Exception as e:
                            st.error(f"‚ùå Erro ao gerar PDF: {str(e)}")
                else:
                    st.warning("‚ö†Ô∏è Execute o resumo das an√°lises primeiro.")

    except Exception as e:
        st.error(f"‚ùå Erro ao processar o dataset: {str(e)}")
        st.write("**Detalhes do erro:**")
        st.code(str(e))

else:
    # P√°gina inicial
    st.info("üëÜ **Fa√ßa upload de um arquivo CSV na barra lateral para come√ßar a an√°lise inteligente.**")

    st.markdown("""
    ## üöÄ **Agente Aut√¥nomo de An√°lise de Dados Inteligente e Adaptativa**

    ### üß† **Funcionalidades Inteligentes:**

    #### **üí° Mem√≥ria Persistente**
    - üîÑ Armazena todas as conclus√µes de an√°lises realizadas
    - üìö Hist√≥rico completo de insights descobertos
    - ü§ñ Capacidade de consultar an√°lises anteriores
    - üìä Resumos inteligentes das descobertas

    #### **üìà An√°lises Avan√ßadas Dispon√≠veis:**
    - ‚úÖ **Estat√≠sticas Descritivas:** An√°lise completa com insights autom√°ticos
    - ‚úÖ **Correla√ß√µes:** Mapas de calor com identifica√ß√£o de rela√ß√µes significativas
    - ‚úÖ **Padr√µes Temporais:** Detec√ß√£o de tend√™ncias e comportamentos ao longo do tempo
    - ‚úÖ **Clustering Inteligente:** Agrupamento autom√°tico com otimiza√ß√£o de par√¢metros
    - ‚úÖ **Detec√ß√£o de Outliers:** M√∫ltiplos m√©todos (Isolation Forest, IQR, Z-score)
    - ‚úÖ **An√°lise de Frequ√™ncias:** Valores mais/menos comuns em dados categ√≥ricos

    #### **üéØ Recursos Especiais:**
    - ü§ñ **Interpreta√ß√£o de Linguagem Natural Aprimorada:** Compreende perguntas em portugu√™s
    - üìÑ **Exporta√ß√£o PDF:** Gera relat√≥rios profissionais automaticamente
    - ‚ö° **Resumo Completo:** Executa e compila todas as an√°lises em texto
    - üé® **Interface Otimizada:** Foco na usabilidade e resultados
    - üîß **Otimiza√ß√£o de Performance:** Amostragem inteligente para datasets grandes
    - üß† **Intelig√™ncia Adaptativa:** Sugere an√°lises com base na estrutura do dataset

    ### üìù **Exemplos de Perguntas Inteligentes:**

    - *"Mostre correla√ß√µes entre vari√°veis"*
    - *"Mostre padr√µes temporais nos dados"*
    - *"Fa√ßa clustering autom√°tico dos dados"*
    - *"Detecte outliers com m√∫ltiplos m√©todos"*
    - *"Mostre valores mais frequentes"*
    - *"Qual sua mem√≥ria de an√°lises?"*

    ---


    ‚úÖ **Mem√≥ria do Agente** | ‚úÖ **Padr√µes Temporais** | ‚úÖ **Clustering** | ‚úÖ **Exporta√ß√£o PDF** | ‚úÖ **Intelig√™ncia Adaptativa**
    """)


# Rodap√©
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666; font-size: 14px;'>
ü§ñ <strong>Agente Aut√¥nomo de An√°lise de Dados</strong><br>
Desenvolvido para o <strong>Desafio I2A2 Academy</strong> por Ana Manuella Ribeiro | Setembro 2025<br>
</div>
""", unsafe_allow_html=True)


