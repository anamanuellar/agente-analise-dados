import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import io
import json
from datetime import datetime
import base64
import re
import streamlit as st
import google.generativeai as genai
from typing import Dict, List, Any, Tuple

class GeminiAgent:
    """Agente que USA Google Gemini como c√©rebro do sistema"""
    
    def __init__(self, model_name="gemini-2.0-flash-exp"):  # Modelo mais recente
        self.model_name = model_name
        self.model = None
        self.conversation_history = []
        self.dataset_context = {}
        
        self.safety_settings = [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH", 
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            }
        ]
        
        self.generation_config = {
            "temperature": 0.3,
            "top_p": 0.8,
            "top_k": 40,
            "max_output_tokens": 4000,
        }
    
    def configure_gemini(self, api_key: str):
        """Configura o Gemini com a API Key fornecida."""
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(
            self.model_name,
            safety_settings=self.safety_settings,
            generation_config=self.generation_config
        )

    def _call_gemini(self, prompt: str, system_context: str = "") -> str:
        """Chama o Google Gemini para an√°lise"""
        if not self.model:
            return self._fallback_response("Gemini n√£o configurado. Por favor, forne√ßa a API Key.")

        try:
            full_prompt = f"{system_context}\n\n{prompt}" if system_context else prompt
            response = self.model.generate_content(full_prompt)
            return response.text
            
        except Exception as e:
            st.error(f"Erro no Gemini: {e}")
            return self._fallback_response(prompt)
    
    def _fallback_response(self, prompt: str) -> str:
        """Resposta de fallback quando Gemini falha"""
        return f"""
**[Modo Fallback - Gemini Temporariamente Indispon√≠vel]**

Recebi sua pergunta: "{prompt}"

Esta √© uma resposta de fallback. Para an√°lise completa com IA:
1. Verifique sua API key do Gemini
2. Confirme conex√£o com internet
3. Tente novamente em alguns instantes

As funcionalidades b√°sicas de an√°lise continuam funcionando normalmente.
        """
    
    def analyze_dataset_initially(self, df: pd.DataFrame) -> Dict[str, Any]:
        """An√°lise inicial PROFUNDA e anal√≠tica do dataset usando Gemini"""
        
        # Primeiro, calcular estat√≠sticas avan√ßadas para o prompt
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        categorical_cols = df.select_dtypes(include=["object"]).columns
        
        # An√°lise de correla√ß√µes se houver colunas num√©ricas
        correlation_summary = ""
        if len(numeric_cols) >= 2:
            corr_matrix = df[numeric_cols].corr()
            high_corr = []
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    corr_val = corr_matrix.iloc[i, j]
                    if not np.isnan(corr_val) and abs(corr_val) > 0.5:
                        high_corr.append(f"{corr_matrix.columns[i]} ‚Üî {corr_matrix.columns[j]}: {corr_val:.3f}")
            correlation_summary = f"Correla√ß√µes significativas encontradas: {len(high_corr)} pares com |r| > 0.5"
        
        # An√°lise de distribui√ß√µes
        distribution_summary = ""
        if len(numeric_cols) > 0:
            skew_analysis = []
            for col in numeric_cols[:5]:  # Primeiras 5 colunas
                skewness = df[col].skew()
                if abs(skewness) > 1:
                    skew_analysis.append(f"{col}: assimetria alta ({skewness:.2f})")
                elif abs(skewness) > 0.5:
                    skew_analysis.append(f"{col}: assimetria moderada ({skewness:.2f})")
            distribution_summary = f"Distribui√ß√µes: {'; '.join(skew_analysis) if skew_analysis else 'distribui√ß√µes aproximadamente normais'}"
        
        # An√°lise de qualidade detalhada
        missing_analysis = []
        for col in df.columns:
            missing_pct = (df[col].isnull().sum() / len(df)) * 100
            if missing_pct > 0:
                missing_analysis.append(f"{col}: {missing_pct:.1f}% faltante")
        
        system_context = """Voc√™ √© um Senior Data Scientist com 15+ anos de experi√™ncia em an√°lise explorat√≥ria de dados. 
        Sua especialidade √© identificar padr√µes, anomalias e oportunidades de insight em datasets complexos.
        Forne√ßa an√°lises T√âCNICAS, ESPEC√çFICAS e QUANTITATIVAS. Evite respostas gen√©ricas."""
        
        prompt = f"""
        DATASET PARA AN√ÅLISE PROFUNDA:

        üìä ESTRUTURA:
        - {df.shape[0]:,} registros √ó {df.shape[1]} vari√°veis
        - Densidade: {((df.shape[0] * df.shape[1] - df.isnull().sum().sum()) / (df.shape[0] * df.shape[1]) * 100):.1f}%
        - Mem√≥ria: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB

        üìà COMPOSI√á√ÉO DOS DADOS:
        - Num√©ricas: {list(numeric_cols)} ({len(numeric_cols)} colunas)
        - Categ√≥ricas: {list(categorical_cols)} ({len(categorical_cols)} colunas)

        üìã AMOSTRA REPRESENTATIVA:
        {df.head(3).to_string()}

        üìä ESTAT√çSTICAS QUANTITATIVAS:
        {df.describe().to_string() if len(numeric_cols) > 0 else "Sem colunas num√©ricas para an√°lise estat√≠stica"}

        üîó AN√ÅLISE DE CORRELA√á√ïES:
        {correlation_summary}

        üìê AN√ÅLISE DE DISTRIBUI√á√ïES:
        {distribution_summary}

        üîç QUALIDADE DOS DADOS:
        - Valores faltantes: {'; '.join(missing_analysis[:10]) if missing_analysis else 'Dataset completo'}
        - Duplicatas: {df.duplicated().sum():,} registros
        # Calcular primeiro, depois formatar:
        variability_text = "N/A"
        if len(numeric_cols) > 0:
            try:
                cv_mean = (df[numeric_cols].std() / df[numeric_cols].mean()).mean()
                if not np.isnan(cv_mean) and np.isfinite(cv_mean):
                    variability_text = f"{cv_mean:.3f}"
            except:
                variability_text = "N/A"
        - Variabilidade: CV m√©dio = {variability_text}

        FORNE√áA UMA AN√ÅLISE ESTRUTURADA E T√âCNICA:

        ## IDENTIFICA√á√ÉO DO DOM√çNIO
        Com base nos nomes das colunas, distribui√ß√µes e padr√µes, identifique especificamente o tipo de dataset (ex: transa√ß√µes financeiras, dados de marketing, logs de sistema, etc.) e justifique sua conclus√£o.

        ## AVALIA√á√ÉO T√âCNICA DE QUALIDADE
        Avalie objetivamente: completude, consist√™ncia, outliers potenciais, balanceamento (se aplic√°vel). Use n√∫meros espec√≠ficos.

        ## CARACTER√çSTICAS ANAL√çTICAS PRINCIPAIS  
        - [Caracter√≠stica 1: padr√£o espec√≠fico identificado com evid√™ncias]
        - [Caracter√≠stica 2: distribui√ß√£o ou correla√ß√£o relevante]
        - [Caracter√≠stica 3: aspecto de qualidade ou estrutura importante]

        ## AN√ÅLISES PRIORIT√ÅRIAS RECOMENDADAS
        - [An√°lise 1: t√©cnica espec√≠fica e por que √© cr√≠tica para este dataset]
        - [An√°lise 2: m√©todo estat√≠stico recomendado e valor esperado]
        - [An√°lise 3: explora√ß√£o direcionada baseada nos padr√µes identificados]

        ## HIP√ìTESES E INSIGHTS POTENCIAIS
        - [Hip√≥tese 1: baseada em evid√™ncias dos dados observados]
        - [Hip√≥tese 2: padr√£o ou anomalia que merece investiga√ß√£o]
        - [Hip√≥tese 3: oportunidade de descoberta espec√≠fica]

        Seja T√âCNICO, ESPEC√çFICO e baseado em EVID√äNCIAS dos dados mostrados.
        """
        
        response = self._call_gemini(prompt, system_context)
        
        # Parsing mais robusto da resposta
        analysis_result = self._parse_detailed_analysis(response, df)
        
        self.dataset_context = analysis_result
        self._add_to_gemini_memory("initial_analysis", analysis_result)
        
        return analysis_result
    
    def _parse_detailed_analysis(self, response: str, df: pd.DataFrame) -> Dict[str, Any]:
        """Parse robusto da an√°lise detalhada"""
        try:
            # Extrair se√ß√µes com melhor parsing
            dataset_type = self._extract_robust_section(response, ["IDENTIFICA√á√ÉO DO DOM√çNIO", "DOM√çNIO"], ["AVALIA√á√ÉO", "QUALIDADE"])
            data_quality = self._extract_robust_section(response, ["AVALIA√á√ÉO", "QUALIDADE"], ["CARACTER√çSTICAS", "ANAL√çTICAS"])
            key_characteristics = self._extract_robust_list(response, ["CARACTER√çSTICAS", "ANAL√çTICAS"], ["AN√ÅLISES", "PRIORIT√ÅRIAS", "RECOMENDADAS"])
            recommended_analyses = self._extract_robust_list(response, ["AN√ÅLISES", "RECOMENDADAS", "PRIORIT√ÅRIAS"], ["HIP√ìTESES", "INSIGHTS"])
            potential_insights = self._extract_robust_list(response, ["HIP√ìTESES", "INSIGHTS"], None)
            
            # Garantir conte√∫do m√≠nimo de qualidade
            if not dataset_type or len(dataset_type.strip()) < 10:
                dataset_type = self._generate_fallback_domain_analysis(df)
            
            if not key_characteristics or len(key_characteristics) < 2:
                key_characteristics = self._generate_fallback_characteristics(df)
            
            if not recommended_analyses or len(recommended_analyses) < 2:
                recommended_analyses = self._generate_fallback_recommendations(df)
                
            if not potential_insights or len(potential_insights) < 2:
                potential_insights = self._generate_fallback_insights(df)
            
            return {
                "dataset_type": dataset_type.strip(),
                "data_quality": data_quality.strip() or self._generate_fallback_quality_analysis(df),
                "key_characteristics": key_characteristics,
                "recommended_analyses": recommended_analyses,
                "potential_insights": potential_insights,
                "full_response": response
            }
            
        except Exception as e:
            # Fallback com an√°lise robusta pr√≥pria
            return {
                "dataset_type": self._generate_fallback_domain_analysis(df),
                "data_quality": self._generate_fallback_quality_analysis(df),
                "key_characteristics": self._generate_fallback_characteristics(df),
                "recommended_analyses": self._generate_fallback_recommendations(df),
                "potential_insights": self._generate_fallback_insights(df),
                "full_response": response
            }
    
    def _extract_robust_section(self, text: str, start_markers: List[str], end_markers: List[str]) -> str:
        """Extra√ß√£o robusta de se√ß√µes com m√∫ltiplos marcadores"""
        for start_marker in start_markers:
            try:
                start_idx = text.upper().find(start_marker.upper())
                if start_idx != -1:
                    start_idx = start_idx + len(start_marker)
                    
                    end_idx = len(text)
                    if end_markers:
                        for end_marker in end_markers:
                            temp_end = text.upper().find(end_marker.upper(), start_idx)
                            if temp_end != -1 and temp_end < end_idx:
                                end_idx = temp_end
                    
                    section = text[start_idx:end_idx].strip()
                    # Limpar marcadores markdown
                    section = section.replace("##", "").replace("#", "").strip()
                    
                    if len(section) > 10:  # Se√ß√£o substancial
                        return section
            except:
                continue
        return ""
    
    def _extract_robust_list(self, text: str, start_markers: List[str], end_markers: List[str]) -> List[str]:
        """Extra√ß√£o robusta de listas"""
        section = self._extract_robust_section(text, start_markers, end_markers)
        if not section:
            return []
        
        items = []
        lines = section.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith(('-', '‚Ä¢', '*', '1.', '2.', '3.', '4.', '5.')):
                item = line.lstrip('-‚Ä¢*123456789. ').strip()
                if len(item) > 15:  # Item substancial
                    items.append(item)
        
        return items[:5]  # M√°ximo 5 itens
    
    def _generate_fallback_domain_analysis(self, df: pd.DataFrame) -> str:
        """Gera an√°lise de dom√≠nio robusta baseada nas colunas"""
        cols_lower = [col.lower() for col in df.columns]
        
        # Detec√ß√£o por padr√µes de nomes
        if any(word in ' '.join(cols_lower) for word in ['transaction', 'amount', 'fraud', 'class']):
            return "Dataset de Detec√ß√£o de Fraude - baseado em colunas de transa√ß√£o e classifica√ß√£o"
        elif any(word in ' '.join(cols_lower) for word in ['price', 'sales', 'revenue', 'customer']):
            return "Dataset de Vendas/E-commerce - baseado em vari√°veis comerciais"
        elif any(word in ' '.join(cols_lower) for word in ['time', 'timestamp', 'date']):
            return "Dataset Temporal - cont√©m componentes de s√©rie temporal"
        elif len(df.select_dtypes(include=[np.number]).columns) > len(df.columns) * 0.7:
            return "Dataset Quantitativo - predominantemente num√©rico para an√°lise estat√≠stica"
        else:
            return f"Dataset Misto - {len(df.select_dtypes(include=[np.number]).columns)} vari√°veis num√©ricas e {len(df.select_dtypes(include=['object']).columns)} categ√≥ricas"
    
    def _generate_fallback_quality_analysis(self, df: pd.DataFrame) -> str:
        """Gera an√°lise de qualidade robusta"""
        completeness = ((df.shape[0] * df.shape[1] - df.isnull().sum().sum()) / (df.shape[0] * df.shape[1]) * 100)
        duplicates = df.duplicated().sum()
        
        quality_score = "EXCELENTE" if completeness > 95 and duplicates == 0 else \
                       "BOA" if completeness > 90 else \
                       "MODERADA" if completeness > 80 else "BAIXA"
        
        return f"Qualidade {quality_score}: {completeness:.1f}% completo, {duplicates:,} duplicatas, {df.shape[0]:,} registros v√°lidos para an√°lise"
    
    def _generate_fallback_characteristics(self, df: pd.DataFrame) -> List[str]:
        """Gera caracter√≠sticas robustas baseadas na an√°lise dos dados"""
        characteristics = []
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            skew_analysis = df[numeric_cols].skew()
            highly_skewed = sum(abs(skew_analysis) > 1)
            characteristics.append(f"Distribui√ß√µes: {highly_skewed} de {len(numeric_cols)} vari√°veis num√©ricas apresentam assimetria alta")
            
            # An√°lise de variabilidade
            cv = (df[numeric_cols].std() / df[numeric_cols].mean()).mean()
            if cv > 1:
                characteristics.append(f"Variabilidade alta: coeficiente de varia√ß√£o m√©dio de {cv:.2f} indica dados heterog√™neos")
            
            # An√°lise de correla√ß√µes
            if len(numeric_cols) >= 2:
                corr_matrix = df[numeric_cols].corr()
                high_corr_count = sum(sum(abs(corr_matrix.values) > 0.7) - len(corr_matrix)) // 2
                characteristics.append(f"Estrutura de correla√ß√£o: {high_corr_count} pares de vari√°veis altamente correlacionadas")
        
        # An√°lise categ√≥rica
        categorical_cols = df.select_dtypes(include=['object']).columns
        if len(categorical_cols) > 0:
            high_cardinality = sum(df[col].nunique() > 50 for col in categorical_cols)
            if high_cardinality > 0:
                characteristics.append(f"Cardinalidade: {high_cardinality} vari√°veis categ√≥ricas com alta diversidade (>50 valores √∫nicos)")
        
        return characteristics[:3] if characteristics else ["Dataset com estrutura padr√£o para an√°lise explorat√≥ria"]
    
    def _generate_fallback_recommendations(self, df: pd.DataFrame) -> List[str]:
        """Gera recomenda√ß√µes robustas baseadas na estrutura dos dados"""
        recommendations = []
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) >= 2:
            recommendations.append(f"An√°lise de correla√ß√£o entre {len(numeric_cols)} vari√°veis num√©ricas para identificar relacionamentos lineares")
        
        if len(numeric_cols) > 0:
            recommendations.append("Detec√ß√£o de outliers multivariada usando Isolation Forest para identificar anomalias")
        
        if df.shape[0] > 1000:
            recommendations.append("Clustering hier√°rquico ou K-means para segmenta√ß√£o e identifica√ß√£o de padr√µes latentes")
        
        # An√°lise temporal se detectada
        time_cols = [col for col in df.columns if 'time' in col.lower() or 'date' in col.lower()]
        if time_cols:
            recommendations.append(f"An√°lise de s√©ries temporais na coluna {time_cols[0]} para identificar tend√™ncias e sazonalidade")
        
        return recommendations[:3] if recommendations else ["An√°lise explorat√≥ria sistem√°tica das vari√°veis principais"]
    
    def _generate_fallback_insights(self, df: pd.DataFrame) -> List[str]:
        """Gera insights potenciais robustos"""
        insights = []
        
        # Insights baseados na estrutura
        if df.shape[0] > 10000:
            insights.append(f"Grande volume de dados ({df.shape[0]:,} registros) permite an√°lises estat√≠sticas robustas e modelagem preditiva")
        
        # Insights sobre balanceamento
        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols[:2]:
            value_counts = df[col].value_counts()
            if len(value_counts) == 2:  # Bin√°ria
                balance_ratio = value_counts.min() / value_counts.max()
                if balance_ratio < 0.1:
                    insights.append(f"Forte desbalanceamento na vari√°vel {col} ({balance_ratio:.2f}) sugere necessidade de t√©cnicas de balanceamento")
        
        # Insights sobre missing data
        missing_cols = [col for col in df.columns if df[col].isnull().sum() > 0]
        if len(missing_cols) > df.shape[1] * 0.3:
            insights.append(f"Padr√£o de dados faltantes em {len(missing_cols)} vari√°veis pode indicar processo de coleta estruturado")
        
        return insights[:3] if insights else ["Potencial para descoberta de padr√µes n√£o √≥bvios atrav√©s de an√°lise multivariada"]
    
    def process_user_query(self, user_query: str, df: pd.DataFrame) -> Tuple[str, Dict]:
        """Processa query do usu√°rio usando Gemini para interpreta√ß√£o e execu√ß√£o"""
        
        self.conversation_history.append({
            "role": "user",
            "content": user_query,
            "timestamp": datetime.now()
        })
        
        # FASE 1: GEMINI INTERPRETA A QUERY E CRIA PLANO
        analysis_plan = self._gemini_create_analysis_plan(user_query, df)
        
        # FASE 2: EXECUTAR AN√ÅLISE BASEADA NO PLANO
        analysis_results = self._execute_gemini_analysis_plan(analysis_plan, df)
        
        # FASE 3: GEMINI GERA RESPOSTA FINAL
        final_response = self._gemini_generate_final_response(user_query, analysis_plan, analysis_results, df)
        
        # FASE 4: CRIAR VISUALIZA√á√ÉO SE NECESS√ÅRIO - CORRIGIDO
        visualization = self._create_visualization_if_needed(analysis_plan, df, analysis_results)
        
        self.conversation_history.append({
            "role": "assistant",
            "content": final_response,
            "analysis_plan": analysis_plan,
            "results": analysis_results,
            "visualization": visualization,
            "timestamp": datetime.now()
        })
        
        self._add_to_gemini_memory("user_query", {
            "query": user_query,
            "response": final_response,
            "plan": analysis_plan,
            "results": analysis_results,
            "visualization": visualization
        })
        
        return final_response, visualization
    
    def _gemini_create_analysis_plan(self, user_query: str, df: pd.DataFrame) -> Dict:
        """Gemini cria plano de an√°lise estruturado"""
        
        system_context = """Voc√™ √© um agente especialista em an√°lise de dados. Interprete perguntas do usu√°rio e crie planos estruturados de an√°lise em formato JSON."""
        
        prompt = f"""
        **Pergunta do Usu√°rio:** "{user_query}"

        **Contexto do Dataset:**
        - Colunas dispon√≠veis: {list(df.columns)}
        - Colunas num√©ricas: {list(df.select_dtypes(include=[np.number]).columns)}
        - Colunas categ√≥ricas: {list(df.select_dtypes(include=["object"]).columns)}

        **Sua Tarefa:**
        Crie um plano de an√°lise em formato JSON para responder √† pergunta do usu√°rio. O JSON deve ter a seguinte estrutura:
        {{
            "analysis_type": "[tipo_da_analise]",
            "columns_to_use": ["coluna1", "coluna2"],
            "requires_visualization": [true/false],
            "visualization_type": "[tipo_do_grafico]"
        }}

        **Tipos de An√°lise V√°lidos:**
        - `descriptive_statistics`: Para perguntas sobre m√©dia, mediana, desvio padr√£o, etc.
        - `correlation_matrix`: Para perguntas sobre correla√ß√£o entre vari√°veis.
        - `distribution_plot`: Para perguntas sobre a distribui√ß√£o de uma vari√°vel (histograma).
        - `outlier_detection`: Para perguntas sobre outliers ou valores at√≠picos.
        - `clustering`: Para perguntas sobre agrupamentos ou segmenta√ß√£o.
        - `frequency_analysis`: Para perguntas sobre valores mais/menos frequentes.
        - `temporal_analysis`: Para perguntas sobre padr√µes temporais.
        - `balance_analysis`: Para perguntas sobre balanceamento de classes.
        - `general_query`: Para perguntas gerais que n√£o se encaixam nas categorias acima.

        **Tipos de Gr√°fico V√°lidos:**
        - `histogram`: Para `distribution_plot`.
        - `heatmap`: Para `correlation_matrix`.
        - `boxplot`: Para `outlier_detection`.
        - `scatterplot`: Para `clustering`.
        - `pie_chart`: Para `balance_analysis`.
        - `line_chart`: Para `temporal_analysis`.
        - `bar_chart`: Para `frequency_analysis`.

        **Exemplo de Resposta:**
        {{
            "analysis_type": "correlation_matrix",
            "columns_to_use": {list(df.select_dtypes(include=[np.number]).columns)[:2]},
            "requires_visualization": true,
            "visualization_type": "heatmap"
        }}

        **Sua Resposta (apenas o JSON):**
        """
        
        response = self._call_gemini(prompt, system_context)
        
        try:
            json_response = response.strip().replace("```json", "").replace("```", "").strip()
            plan = json.loads(json_response)
        except (json.JSONDecodeError, KeyError):
            plan = {
                "analysis_type": "general_query",
                "columns_to_use": [],
                "requires_visualization": False,
                "visualization_type": "none"
            }
        
        return plan

    def _execute_gemini_analysis_plan(self, plan: Dict, df: pd.DataFrame) -> Dict:
        """Executa o plano de an√°lise gerando e executando c√≥digo com Gemini."""
        
        analysis_type = plan.get("analysis_type", "general_query")
        columns = plan.get("columns_to_use", [])
        
        system_context = """Voc√™ √© um especialista em Python para an√°lise de dados. Gere c√≥digo Python para realizar a an√°lise solicitada. O c√≥digo deve ser completo, funcional e imprimir os resultados em formato de texto."""
        
        prompt = f"""
        **Plano de An√°lise:**
        - Tipo: {analysis_type}
        - Colunas: {columns}

        **Sua Tarefa:**
        Gere o c√≥digo Python completo para realizar esta an√°lise no DataFrame `df`. O c√≥digo deve:
        1. Usar as bibliotecas `pandas`, `numpy`, `matplotlib`, `seaborn`, `sklearn`.
        2. Realizar a an√°lise solicitada nas colunas especificadas.
        3. Imprimir os resultados da an√°lise em formato de texto claro e informativo.
        4. N√£o gere o c√≥digo para criar gr√°ficos, apenas a an√°lise textual.
        5. Se a an√°lise for de clustering, use uma amostra de no m√°ximo 5000 linhas para performance.
        6. Se a an√°lise for de outliers, use IsolationForest, IQR e Z-score.

        **Exemplo de C√≥digo para `descriptive_statistics`:**
        ```python
        desc_stats = df["{columns[0] if columns else 'Amount'}"].describe()
        print("Estat√≠sticas Descritivas:\\n", desc_stats)
        ```

        **Seu C√≥digo Python (apenas o c√≥digo):**
        """
        
        code_to_execute = self._call_gemini(prompt, system_context)
        
        code_to_execute = code_to_execute.strip().replace("```python", "").replace("```", "").strip()
        
        try:
            output_buffer = io.StringIO()
            exec_globals = {
                "df": df,
                "pd": pd,
                "np": np,
                "plt": plt,
                "sns": sns,
                "StandardScaler": StandardScaler,
                "IsolationForest": IsolationForest,
                "KMeans": KMeans,
                "silhouette_score": silhouette_score,
                "print": lambda *args, **kwargs: print(*args, file=output_buffer, **kwargs)
            }
            
            exec(code_to_execute, exec_globals)
            
            text_output = output_buffer.getvalue()
            
            return {"status": "success", "text_output": text_output, "code_executed": code_to_execute}
            
        except Exception as e:
            return {"status": "error", "error_message": str(e), "code_executed": code_to_execute}

    def _gemini_generate_final_response(self, user_query: str, plan: Dict, results: Dict, df: pd.DataFrame) -> str:
        """Gera a resposta final em linguagem natural com base nos resultados."""
        
        system_context = """Voc√™ √© um especialista em an√°lise de dados apresentando resultados para um cliente. Seja claro, conciso e foque em insights de neg√≥cio."""
        
        prompt = f"""
        **Pergunta do Cliente:** "{user_query}"

        **An√°lise Realizada:**
        - Tipo: {plan.get("analysis_type")}
        - Colunas: {plan.get("columns_to_use")}

        **Resultados da An√°lise:**
        ```
        {results.get("text_output")}
        ```

        **Sua Tarefa:**
        Com base nos resultados, escreva uma resposta clara e informativa para o cliente. A resposta deve:
        1. Explicar o que foi analisado.
        2. Resumir os principais resultados.
        3. Fornecer insights pr√°ticos e recomenda√ß√µes.
        4. Usar linguagem acess√≠vel, evitando jarg√µes t√©cnicos excessivos.
        
        **Sua Resposta:**
        """
        
        return self._call_gemini(prompt, system_context)

    def _create_visualization_if_needed(self, plan: Dict, df: pd.DataFrame, analysis_results: Dict) -> Dict:
        """VERS√ÉO CORRIGIDA - Gera visualiza√ß√£o funcionalmente"""
        
        if not plan.get("requires_visualization"): 
            return None
        
        visualization_type = plan.get("visualization_type")
        columns = plan.get("columns_to_use")
        
        system_context = """Voc√™ √© um especialista em visualiza√ß√£o de dados com Python. 
        Gere c√≥digo Python completo e funcional para criar o gr√°fico solicitado.
        O c√≥digo deve:
        1. Ser sintaticamente correto
        2. Criar uma figura usando plt.subplots()
        3. Usar matplotlib/seaborn para o gr√°fico
        4. Incluir t√≠tulo e labels
        5. Armazenar a figura na vari√°vel 'generated_fig'
        6. Usar plt.tight_layout()
        """
        
        prompt = f"""
        **Plano de Visualiza√ß√£o:**
        - Tipo de Gr√°fico: {visualization_type}
        - Colunas: {columns}

        **Sua Tarefa:**
        Gere c√≥digo Python completo para criar este gr√°fico. O c√≥digo deve:

        1. Criar uma figura: `generated_fig, ax = plt.subplots(figsize=(10, 6))`
        2. Gerar o gr√°fico solicitado no eixo `ax`
        3. Incluir t√≠tulo e r√≥tulos
        4. Terminar com `plt.tight_layout()`
        5. A figura deve estar na vari√°vel `generated_fig`

        **Exemplo para histogram:**
        ```python
        generated_fig, ax = plt.subplots(figsize=(10, 6))
        sns.histplot(df['{columns[0] if columns else 'Amount'}'], kde=True, ax=ax)
        ax.set_title('Distribui√ß√£o de {columns[0] if columns else 'Amount'}')
        ax.set_xlabel('{columns[0] if columns else 'Amount'}')
        ax.set_ylabel('Frequ√™ncia')
        plt.tight_layout()
        ```

        **Seu C√≥digo Python (apenas o c√≥digo, sem coment√°rios):**
        """
        
        code_to_execute = self._call_gemini(prompt, system_context)
        
        # Limpar o c√≥digo
        code_to_execute = code_to_execute.strip().replace("```python", "").replace("```", "").strip()
        
        try:
            # CORRE√á√ÉO PRINCIPAL: usar exec() e depois capturar a vari√°vel
            exec_globals = {
                "df": df,
                "pd": pd,
                "np": np,
                "plt": plt,
                "sns": sns,
                "StandardScaler": StandardScaler,
                "IsolationForest": IsolationForest,
                "KMeans": KMeans,
                "silhouette_score": silhouette_score
            }
            
            # Executar o c√≥digo
            exec(code_to_execute, exec_globals)
            
            # Capturar a figura gerada
            generated_fig = exec_globals.get('generated_fig')
            
            if generated_fig is None:
                return {"status": "error", "error_message": "Figura n√£o foi criada corretamente", "code_executed": code_to_execute}
            
            return {"status": "success", "figure": generated_fig, "code_executed": code_to_execute}
            
        except Exception as e:
            st.error(f"Erro ao gerar visualiza√ß√£o: {e}")
            st.code(code_to_execute, language="python")
            return {"status": "error", "error_message": str(e), "code_executed": code_to_execute}

    def _extract_section(self, text: str, start_marker: str, end_marker: str) -> str:
        """Extrai se√ß√£o de texto entre dois marcadores."""
        try:
            start_index = text.index(start_marker) + len(start_marker)
            if end_marker:
                end_index = text.index(end_marker, start_index)
                return text[start_index:end_index].strip()
            else:
                return text[start_index:].strip()
        except ValueError:
            return ""

    def _extract_list_items(self, text: str, start_marker: str, end_marker: str) -> List[str]:
        """Extrai itens de lista de uma se√ß√£o de texto."""
        section = self._extract_section(text, start_marker, end_marker)
        return [item.strip().lstrip("- ") for item in section.split("\n") if item.strip().startswith("-")]

    def _add_to_gemini_memory(self, analysis_type: str, data: Dict):
        """Adiciona an√°lise √† mem√≥ria do agente Gemini."""
        if "gemini_memory" not in st.session_state:
            st.session_state.gemini_memory = []
        
        st.session_state.gemini_memory.append({
            "type": analysis_type,
            "data": data,
            "timestamp": datetime.now()
        })

    def get_full_memory_summary(self) -> str:
        """Gera um resumo completo de todas as intera√ß√µes e an√°lises na mem√≥ria do Gemini."""
        summary_text = """
ü§ñ **RESUMO COMPLETO DAS AN√ÅLISES - AGENTE AUT√îNOMO COM GEMINI**

"""
        if not st.session_state.gemini_memory:
            return summary_text + "Nenhuma an√°lise realizada ainda."

        for i, entry in enumerate(st.session_state.gemini_memory):
            summary_text += f"\n--- **An√°lise {i+1}: {entry['type'].replace('_', ' ').title()}** ---\n"
            summary_text += f"**Timestamp:** {entry['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\n"
            
            if entry["type"] == "initial_analysis":
                summary_text += f"**An√°lise Inicial do Dataset:**\n{entry['data']['full_response']}\n"
            elif entry["type"] == "user_query":
                summary_text += f"**Pergunta do Usu√°rio:** {entry['data']['query']}\n"
                summary_text += f"**Plano de An√°lise:** {json.dumps(entry['data']['plan'], indent=2)}\n"
                summary_text += f"**Resultados da Execu√ß√£o:**\n```\n{entry['data']['results']['text_output']}\n```\n"
                summary_text += f"**Resposta Final do Gemini:**\n{entry['data']['response']}\n"
            
        return summary_text

    def generate_smart_suggestions(self, df: pd.DataFrame) -> List[str]:
        """Gera sugest√µes inteligentes baseadas no dataset usando LLM"""
        
        system_context = """Voc√™ √© um especialista em an√°lise de dados. Gere 5-6 sugest√µes espec√≠ficas e pr√°ticas de perguntas que o usu√°rio pode fazer sobre seus dados."""
        
        user_prompt = f"""
        DATASET CONTEXT:
        - Tipo: {self.dataset_context.get("dataset_type", "Gen√©rico")}
        - Shape: {df.shape[0]:,} linhas x {df.shape[1]} colunas
        - Colunas num√©ricas: {list(df.select_dtypes(include=[np.number]).columns)[:8]}
        - Colunas categ√≥ricas: {list(df.select_dtypes(include=["object"]).columns)[:8]}
        
        AN√ÅLISES J√Å REALIZADAS:
        {len(self.conversation_history)} intera√ß√µes anteriores
        
        Gere 5-6 sugest√µes espec√≠ficas de perguntas que seriam valiosas para este dataset.
        Formato: "Pergunta espec√≠fica e clara"
        
        Exemplo de formato:
        - "Quais s√£o as correla√ß√µes mais fortes entre as vari√°veis num√©ricas?"
        - "Existem outliers significativos na coluna Amount?"
        """
        
        response = self._call_gemini(user_prompt, system_context)
        
        suggestions = []
        for line in response.split("\n"):
            line = line.strip()
            if line.startswith("-") or line.startswith("‚Ä¢") or line.startswith("*"):
                suggestion = line[1:].strip().strip("\"")
                if suggestion:
                    suggestions.append(suggestion)
        
        return suggestions[:6]

# Fun√ß√µes auxiliares mantidas do c√≥digo original
def get_dataset_info(df):
    """Retorna informa√ß√µes descritivas completas do dataset."""
    info = f"""
üìä **INFORMA√á√ïES DESCRITIVAS DO DATASET**

**Dimens√µes:**
- Linhas: {df.shape[0]:,}
- Colunas: {df.shape[1]}
- Tamanho em mem√≥ria: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB

**Qualidade dos Dados:**
- Valores nulos: {df.isnull().sum().sum():,}
- Valores √∫nicos (m√©dia): {df.nunique().mean():.0f}
- Completude: {((df.shape[0] * df.shape[1] - df.isnull().sum().sum()) / (df.shape[0] * df.shape[1]) * 100):.1f}%

**Tipos de Colunas:**
- Num√©ricas: {len(df.select_dtypes(include=[np.number]).columns)}
- Categ√≥ricas: {len(df.select_dtypes(include=["object", "category"]).columns)}
- Datetime: {len(df.select_dtypes(include=["datetime"]).columns)}

**Estat√≠sticas Gerais:**
- Densidade de dados: {(df.count().sum() / (df.shape[0] * df.shape[1]) * 100):.1f}%
- Variabilidade m√©dia: {df.select_dtypes(include=[np.number]).std().mean():.2f}
"""
    return info

def generate_pdf_report(df, agent: GeminiAgent):
    """Gera relat√≥rio PDF com todas as an√°lises e informa√ß√µes do dataset."""
    pdf_buffer = io.BytesIO()
    
    with PdfPages(pdf_buffer) as pdf:
        # P√°gina 1: Capa e informa√ß√µes do dataset
        fig = plt.figure(figsize=(8.27, 11.69), dpi=100)
        ax = fig.add_subplot(111)
        
        ax.text(0.5, 0.95, "ü§ñ RELAT√ìRIO COMPLETO DE AN√ÅLISE DE DADOS", 
                ha="center", va="top", fontsize=16, fontweight="bold")
        ax.text(0.5, 0.90, "Agente Aut√¥nomo com IA Generativa", 
                ha="center", va="top", fontsize=12)
        ax.text(0.5, 0.87, f"Gerado em: {datetime.now().strftime('%d/%m/%Y %H:%M')}", 
                ha="center", va="top", fontsize=10)
        
        dataset_info = get_dataset_info(df)
        ax.text(0.05, 0.80, dataset_info, ha="left", va="top", fontsize=8, 
                wrap=True, bbox=dict(boxstyle="round,pad=0.3", facecolor="#e0f7fa", edgecolor="#00bcd4"))
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis("off")
        pdf.savefig(fig, bbox_inches="tight")
        plt.close(fig)

        # P√°ginas subsequentes com an√°lises
        if "gemini_memory" in st.session_state and st.session_state.gemini_memory:
            for i, entry in enumerate(st.session_state.gemini_memory):
                fig = plt.figure(figsize=(8.27, 11.69), dpi=100)
                ax = fig.add_subplot(111)
                
                title = f"An√°lise {i+1}: {entry['type'].replace('_', ' ').title()}"
                ax.text(0.05, 0.95, title, ha="left", va="top", fontsize=14, fontweight="bold")
                
                content = ""
                if entry["type"] == "initial_analysis":
                    content = entry["data"]["full_response"]
                elif entry["type"] == "user_query":
                    content = f"Pergunta: {entry['data']['query']}\n\n" \
                              f"Resposta Final:\n{entry['data']['response']}"
                
                ax.text(0.05, 0.90, content[:1500], ha="left", va="top", fontsize=8)
                ax.set_xlim(0, 1)
                ax.set_ylim(0, 1)
                ax.axis("off")
                pdf.savefig(fig, bbox_inches="tight")
                plt.close(fig)

                # Adicionar gr√°ficos se existirem
                if (entry["type"] == "user_query" and 
                    entry["data"]["visualization"] and 
                    entry["data"]["visualization"]["status"] == "success" and
                    entry["data"]["visualization"]["figure"]):
                    
                    fig_vis = entry["data"]["visualization"]["figure"]
                    pdf.savefig(fig_vis, bbox_inches="tight")
                    plt.close(fig_vis)
    
    pdf_buffer.seek(0)
    return pdf_buffer.getvalue()

# Fun√ß√£o para inicializar o agente Gemini
def initialize_gemini_agent():
    """Inicializa o agente Gemini no Streamlit"""
    if 'gemini_agent' not in st.session_state:
        st.session_state.gemini_agent = GeminiAgent()
    
    return st.session_state.gemini_agent

# Fun√ß√£o de compatibilidade para o app.py
def get_adaptive_suggestions(df):
    """Fun√ß√£o de compatibilidade - usa o agente Gemini se dispon√≠vel"""
    if 'gemini_agent' in st.session_state and st.session_state.gemini_agent.model:
        return st.session_state.gemini_agent.generate_smart_suggestions(df)
    else:
        # Fallback para sugest√µes b√°sicas
        return [
            "Mostre estat√≠sticas descritivas das colunas num√©ricas",
            "Analise correla√ß√µes entre as vari√°veis",
            "Detecte outliers nos dados",
            "Fa√ßa clustering autom√°tico",
            "Mostre a distribui√ß√£o da coluna principal",
            "Qual a mem√≥ria do agente?"
        ]

