import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import io
import json
from datetime import datetime
import base64
import re
import streamlit as st
import google.generativeai as genai
from typing import Dict, List, Any, Tuple

# Importa√ß√£o do PdfPages com verifica√ß√£o
try:
    from matplotlib.backends.backend_pdf import PdfPages
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

class GeminiAgent:
    """Agente que USA Google Gemini como c√©rebro do sistema"""
    
    def __init__(self, model_name="gemini-2.0-flash-exp"):
        self.model_name = model_name
        self.model = None
        self.conversation_history = []
        self.dataset_context = {}
        
        self.safety_settings = [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH", 
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            }
        ]
        
        self.generation_config = {
            "temperature": 0.3,
            "top_p": 0.8,
            "top_k": 40,
            "max_output_tokens": 4000,
        }
    
    def configure_gemini(self, api_key: str):
        """Configura o Gemini com a API Key fornecida."""
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(
            self.model_name,
            safety_settings=self.safety_settings,
            generation_config=self.generation_config
        )

    def _call_gemini(self, prompt: str, system_context: str = "") -> str:
        """Chama o Google Gemini para an√°lise"""
        if not self.model:
            return self._fallback_response("Gemini n√£o configurado. Por favor, forne√ßa a API Key.")

        try:
            full_prompt = f"{system_context}\n\n{prompt}" if system_context else prompt
            response = self.model.generate_content(full_prompt)
            return response.text
            
        except Exception as e:
            st.error(f"Erro no Gemini: {e}")
            return self._fallback_response(prompt)
    
    def _fallback_response(self, prompt: str) -> str:
        """Resposta de fallback quando Gemini falha"""
        return f"""
**[Modo Fallback - Gemini Temporariamente Indispon√≠vel]**

Recebi sua pergunta: "{prompt}"

Esta √© uma resposta de fallback. Para an√°lise completa com IA:
1. Verifique sua API key do Gemini
2. Confirme conex√£o com internet
3. Tente novamente em alguns instantes

As funcionalidades b√°sicas de an√°lise continuam funcionando normalmente.
        """
    
    def analyze_dataset_initially(self, df: pd.DataFrame) -> Dict[str, Any]:
        """An√°lise inicial PROFUNDA e anal√≠tica do dataset usando Gemini"""
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        categorical_cols = df.select_dtypes(include=["object"]).columns
        
        # An√°lise de correla√ß√µes
        correlation_summary = ""
        if len(numeric_cols) >= 2:
            try:
                corr_matrix = df[numeric_cols].corr()
                high_corr = []
                for i in range(len(corr_matrix.columns)):
                    for j in range(i+1, len(corr_matrix.columns)):
                        corr_val = corr_matrix.iloc[i, j]
                        if not np.isnan(corr_val) and abs(corr_val) > 0.5:
                            high_corr.append(f"{corr_matrix.columns[i]} ‚Üî {corr_matrix.columns[j]}: {corr_val:.3f}")
                correlation_summary = f"Correla√ß√µes significativas encontradas: {len(high_corr)} pares com |r| > 0.5"
            except:
                correlation_summary = "Correla√ß√µes: an√°lise requer limpeza de dados"
        else:
            correlation_summary = "Correla√ß√µes: insuficientes colunas num√©ricas"
        
        # An√°lise de distribui√ß√µes
        distribution_summary = ""
        if len(numeric_cols) > 0:
            skew_analysis = []
            for col in numeric_cols[:5]:
                try:
                    skewness = df[col].skew()
                    if not np.isnan(skewness) and np.isfinite(skewness):
                        if abs(skewness) > 1:
                            skew_analysis.append(f"{col}: assimetria alta ({skewness:.2f})")
                        elif abs(skewness) > 0.5:
                            skew_analysis.append(f"{col}: assimetria moderada ({skewness:.2f})")
                except:
                    continue
            
            if skew_analysis:
                distribution_summary = f"Distribui√ß√µes: {'; '.join(skew_analysis)}"
            else:
                distribution_summary = "Distribui√ß√µes: aproximadamente normais"
        
        # An√°lise de qualidade detalhada
        missing_analysis = []
        for col in df.columns:
            missing_pct = (df[col].isnull().sum() / len(df)) * 100
            if missing_pct > 0:
                missing_analysis.append(f"{col}: {missing_pct:.1f}% faltante")
        
        # Calcular variabilidade
        variability_text = "N/A"
        if len(numeric_cols) > 0:
            try:
                cv_mean = (df[numeric_cols].std() / df[numeric_cols].mean()).mean()
                if not np.isnan(cv_mean) and np.isfinite(cv_mean):
                    variability_text = f"{cv_mean:.3f}"
            except:
                variability_text = "N/A"
        
        system_context = """Voc√™ √© um Senior Data Scientist com 15+ anos de experi√™ncia em an√°lise explorat√≥ria de dados. 
        Sua especialidade √© identificar padr√µes, anomalias e oportunidades de insight em datasets complexos.
        Forne√ßa an√°lises T√âCNICAS, ESPEC√çFICAS e QUANTITATIVAS. Evite respostas gen√©ricas."""
        
        # Preparar vari√°veis para o prompt
        stats_text = "Sem colunas num√©ricas para an√°lise estat√≠stica"
        if len(numeric_cols) > 0:
            stats_text = df.describe().to_string()
        
        missing_text = "Dataset completo"
        if missing_analysis:
            missing_text = '; '.join(missing_analysis[:10])
        
        prompt = f"""
        DATASET PARA AN√ÅLISE PROFUNDA:

        üìä ESTRUTURA:
        - {df.shape[0]:,} registros √ó {df.shape[1]} vari√°veis
        - Densidade: {((df.shape[0] * df.shape[1] - df.isnull().sum().sum()) / (df.shape[0] * df.shape[1]) * 100):.1f}%
        - Mem√≥ria: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB

        üìà COMPOSI√á√ÉO DOS DADOS:
        - Num√©ricas: {list(numeric_cols)} ({len(numeric_cols)} colunas)
        - Categ√≥ricas: {list(categorical_cols)} ({len(categorical_cols)} colunas)

        üìã AMOSTRA REPRESENTATIVA:
        {df.head(3).to_string()}

        üìä ESTAT√çSTICAS QUANTITATIVAS:
        {stats_text}

        üîó AN√ÅLISE DE CORRELA√á√ïES:
        {correlation_summary}

        üìê AN√ÅLISE DE DISTRIBUI√á√ïES:
        {distribution_summary}

        üîç QUALIDADE DOS DADOS:
        - Valores faltantes: {missing_text}
        - Duplicatas: {df.duplicated().sum():,} registros
        - Variabilidade: CV m√©dio = {variability_text}

        FORNE√áA UMA AN√ÅLISE ESTRUTURADA E T√âCNICA:

        ## IDENTIFICA√á√ÉO DO DOM√çNIO
        Com base nos nomes das colunas, distribui√ß√µes e padr√µes, identifique especificamente o tipo de dataset.

        ## AVALIA√á√ÉO T√âCNICA DE QUALIDADE
        Avalie objetivamente: completude, consist√™ncia, outliers potenciais, balanceamento.

        ## CARACTER√çSTICAS ANAL√çTICAS PRINCIPAIS  
        - [Caracter√≠stica 1: padr√£o espec√≠fico identificado com evid√™ncias]
        - [Caracter√≠stica 2: distribui√ß√£o ou correla√ß√£o relevante]
        - [Caracter√≠stica 3: aspecto de qualidade ou estrutura importante]

        ## AN√ÅLISES PRIORIT√ÅRIAS RECOMENDADAS
        - [An√°lise 1: t√©cnica espec√≠fica e por que √© cr√≠tica para este dataset]
        - [An√°lise 2: m√©todo estat√≠stico recomendado e valor esperado]
        - [An√°lise 3: explora√ß√£o direcionada baseada nos padr√µes identificados]

        ## HIP√ìTESES E INSIGHTS POTENCIAIS
        - [Hip√≥tese 1: baseada em evid√™ncias dos dados observados]
        - [Hip√≥tese 2: padr√£o ou anomalia que merece investiga√ß√£o]
        - [Hip√≥tese 3: oportunidade de descoberta espec√≠fica]

        Seja T√âCNICO, ESPEC√çFICO e baseado em EVID√äNCIAS dos dados mostrados.
        """
        
        response = self._call_gemini(prompt, system_context)
        
        # Parsing da resposta
        analysis_result = self._parse_detailed_analysis(response, df)
        
        self.dataset_context = analysis_result
        self._add_to_gemini_memory("initial_analysis", analysis_result)
        
        return analysis_result
    
    def _parse_detailed_analysis(self, response: str, df: pd.DataFrame) -> Dict[str, Any]:
        """Parse robusto da an√°lise detalhada"""
        try:
            dataset_type = self._extract_robust_section(response, ["IDENTIFICA√á√ÉO DO DOM√çNIO", "DOM√çNIO"], ["AVALIA√á√ÉO", "QUALIDADE"])
            data_quality = self._extract_robust_section(response, ["AVALIA√á√ÉO", "QUALIDADE"], ["CARACTER√çSTICAS", "ANAL√çTICAS"])
            key_characteristics = self._extract_robust_list(response, ["CARACTER√çSTICAS", "ANAL√çTICAS"], ["AN√ÅLISES", "PRIORIT√ÅRIAS", "RECOMENDADAS"])
            recommended_analyses = self._extract_robust_list(response, ["AN√ÅLISES", "RECOMENDADAS", "PRIORIT√ÅRIAS"], ["HIP√ìTESES", "INSIGHTS"])
            potential_insights = self._extract_robust_list(response, ["HIP√ìTESES", "INSIGHTS"], None)
            
            # Garantir conte√∫do m√≠nimo de qualidade
            if not dataset_type or len(dataset_type.strip()) < 10:
                dataset_type = self._generate_fallback_domain_analysis(df)
            
            if not key_characteristics or len(key_characteristics) < 2:
                key_characteristics = self._generate_fallback_characteristics(df)
            
            if not recommended_analyses or len(recommended_analyses) < 2:
                recommended_analyses = self._generate_fallback_recommendations(df)
                
            if not potential_insights or len(potential_insights) < 2:
                potential_insights = self._generate_fallback_insights(df)
            
            return {
                "dataset_type": dataset_type.strip(),
                "data_quality": data_quality.strip() or self._generate_fallback_quality_analysis(df),
                "key_characteristics": key_characteristics,
                "recommended_analyses": recommended_analyses,
                "potential_insights": potential_insights,
                "full_response": response
            }
            
        except Exception as e:
            return {
                "dataset_type": self._generate_fallback_domain_analysis(df),
                "data_quality": self._generate_fallback_quality_analysis(df),
                "key_characteristics": self._generate_fallback_characteristics(df),
                "recommended_analyses": self._generate_fallback_recommendations(df),
                "potential_insights": self._generate_fallback_insights(df),
                "full_response": response
            }
    
    def process_user_query(self, user_query: str, df: pd.DataFrame) -> Tuple[str, Dict]:
        """Processa query do usu√°rio usando Gemini"""
        
        self.conversation_history.append({
            "role": "user",
            "content": user_query,
            "timestamp": datetime.now()
        })
        
        # FASE 1: GEMINI INTERPRETA A QUERY E CRIA PLANO
        analysis_plan = self._gemini_create_analysis_plan(user_query, df)
        
        # FASE 2: EXECUTAR AN√ÅLISE BASEADA NO PLANO
        analysis_results = self._execute_gemini_analysis_plan(analysis_plan, df)
        
        # FASE 3: GEMINI GERA RESPOSTA FINAL
        final_response = self._gemini_generate_final_response(user_query, analysis_plan, analysis_results, df)
        
        # FASE 4: CRIAR VISUALIZA√á√ÉO SE NECESS√ÅRIO
        visualization = self._create_visualization_if_needed(analysis_plan, df, analysis_results)
        
        self.conversation_history.append({
            "role": "assistant",
            "content": final_response,
            "analysis_plan": analysis_plan,
            "results": analysis_results,
            "visualization": visualization,
            "timestamp": datetime.now()
        })
        
        self._add_to_gemini_memory("user_query", {
            "query": user_query,
            "response": final_response,
            "plan": analysis_plan,
            "results": analysis_results,
            "visualization": visualization
        })
        
        return final_response, visualization
    
    def _gemini_create_analysis_plan(self, user_query: str, df: pd.DataFrame) -> Dict:
        """Gemini cria plano de an√°lise estruturado"""
        
        system_context = """Voc√™ √© um agente especialista em an√°lise de dados. Interprete perguntas do usu√°rio e crie planos estruturados de an√°lise em formato JSON."""
        
        prompt = f"""
        **Pergunta do Usu√°rio:** "{user_query}"

        **Contexto do Dataset:**
        - Colunas dispon√≠veis: {list(df.columns)}
        - Colunas num√©ricas: {list(df.select_dtypes(include=[np.number]).columns)}
        - Colunas categ√≥ricas: {list(df.select_dtypes(include=["object"]).columns)}

        **Sua Tarefa:**
        Crie um plano de an√°lise em formato JSON:
        {{
            "analysis_type": "[tipo_da_analise]",
            "columns_to_use": ["coluna1", "coluna2"],
            "requires_visualization": [true/false],
            "visualization_type": "[tipo_do_grafico]"
        }}

        **Tipos de An√°lise V√°lidos:**
        - `descriptive_statistics`: estat√≠sticas b√°sicas
        - `correlation_matrix`: correla√ß√µes
        - `distribution_plot`: distribui√ß√£o de vari√°vel
        - `outlier_detection`: outliers
        - `clustering`: agrupamentos
        - `frequency_analysis`: valores frequentes
        - `temporal_analysis`: padr√µes temporais
        - `balance_analysis`: balanceamento
        - `general_query`: perguntas gerais

        **Sua Resposta (apenas o JSON):**
        """
        
        response = self._call_gemini(prompt, system_context)
        
        try:
            json_response = response.strip().replace("```json", "").replace("```", "").strip()
            plan = json.loads(json_response)
        except (json.JSONDecodeError, KeyError):
            plan = {
                "analysis_type": "general_query",
                "columns_to_use": [],
                "requires_visualization": False,
                "visualization_type": "none"
            }
        
        return plan

    def _execute_gemini_analysis_plan(self, plan: Dict, df: pd.DataFrame) -> Dict:
        """Executa o plano de an√°lise"""
        
        analysis_type = plan.get("analysis_type", "general_query")
        columns = plan.get("columns_to_use", [])
        
        system_context = """Voc√™ √© um especialista em Python para an√°lise de dados. Gere c√≥digo Python para realizar a an√°lise solicitada."""
        
        prompt = f"""
        **Plano de An√°lise:**
        - Tipo: {analysis_type}
        - Colunas: {columns}

        **Sua Tarefa:**
        Gere c√≥digo Python completo para esta an√°lise no DataFrame `df`:
        1. Use pandas, numpy, sklearn
        2. Imprima os resultados em formato claro
        3. N√£o crie gr√°ficos, apenas an√°lise textual

        **Seu C√≥digo Python (apenas o c√≥digo):**
        """
        
        code_to_execute = self._call_gemini(prompt, system_context)
        
        code_to_execute = code_to_execute.strip().replace("```python", "").replace("```", "").strip()
        
        try:
            output_buffer = io.StringIO()
            exec_globals = {
                "df": df,
                "pd": pd,
                "np": np,
                "plt": plt,
                "sns": sns,
                "StandardScaler": StandardScaler,
                "IsolationForest": IsolationForest,
                "KMeans": KMeans,
                "silhouette_score": silhouette_score,
                "print": lambda *args, **kwargs: print(*args, file=output_buffer, **kwargs)
            }
            
            exec(code_to_execute, exec_globals)
            
            text_output = output_buffer.getvalue()
            
            return {"status": "success", "text_output": text_output, "code_executed": code_to_execute}
            
        except Exception as e:
            return {"status": "error", "error_message": str(e), "code_executed": code_to_execute}

    def _gemini_generate_final_response(self, user_query: str, plan: Dict, results: Dict, df: pd.DataFrame) -> str:
        """Gera a resposta final"""
        
        system_context = """Voc√™ √© um especialista em an√°lise de dados apresentando resultados para um cliente. Seja claro, conciso e foque em insights de neg√≥cio."""
        
        prompt = f"""
        **Pergunta do Cliente:** "{user_query}"

        **An√°lise Realizada:**
        - Tipo: {plan.get("analysis_type")}
        - Colunas: {plan.get("columns_to_use")}

        **Resultados da An√°lise:**
        ```
        {results.get("text_output")}
        ```

        **Sua Tarefa:**
        Escreva uma resposta clara:
        1. Explique o que foi analisado
        2. Resumir os principais resultados
        3. Forne√ßa insights pr√°ticos
        4. Use linguagem acess√≠vel
        
        **Sua Resposta:**
        """
        
        return self._call_gemini(prompt, system_context)

    def _create_visualization_if_needed(self, plan: Dict, df: pd.DataFrame, analysis_results: Dict) -> Dict:
        """Gera visualiza√ß√£o se necess√°rio"""
        
        if not plan.get("requires_visualization"): 
            return None
        
        visualization_type = plan.get("visualization_type")
        columns = plan.get("columns_to_use")
        
        system_context = """Voc√™ √© um especialista em visualiza√ß√£o de dados com Python."""
        
        prompt = f"""
        **Plano de Visualiza√ß√£o:**
        - Tipo de Gr√°fico: {visualization_type}
        - Colunas: {columns}

        **Sua Tarefa:**
        Gere c√≥digo Python para criar este gr√°fico:

        1. Criar: `generated_fig, ax = plt.subplots(figsize=(10, 6))`
        2. Gerar o gr√°fico no eixo `ax`
        3. Incluir t√≠tulo e r√≥tulos
        4. Terminar com `plt.tight_layout()`
        5. A figura deve estar na vari√°vel `generated_fig`

        **Seu C√≥digo Python:**
        """
        
        code_to_execute = self._call_gemini(prompt, system_context)
        
        code_to_execute = code_to_execute.strip().replace("```python", "").replace("```", "").strip()
        
        try:
            exec_globals = {
                "df": df,
                "pd": pd,
                "np": np,
                "plt": plt,
                "sns": sns,
                "StandardScaler": StandardScaler,
                "IsolationForest": IsolationForest,
                "KMeans": KMeans,
                "silhouette_score": silhouette_score
            }
            
            exec(code_to_execute, exec_globals)
            
            generated_fig = exec_globals.get('generated_fig')
            
            if generated_fig is None:
                return {"status": "error", "error_message": "Figura n√£o foi criada corretamente", "code_executed": code_to_execute}
            
            return {"status": "success", "figure": generated_fig, "code_executed": code_to_execute}
            
        except Exception as e:
            st.error(f"Erro ao gerar visualiza√ß√£o: {e}")
            st.code(code_to_execute, language="python")
            return {"status": "error", "error_message": str(e), "code_executed": code_to_execute}

    # M√©todos auxiliares
    def _extract_robust_section(self, text: str, start_markers: List[str], end_markers: List[str]) -> str:
        """Extra√ß√£o robusta de se√ß√µes"""
        for start_marker in start_markers:
            try:
                start_idx = text.upper().find(start_marker.upper())
                if start_idx != -1:
                    start_idx = start_idx + len(start_marker)
                    
                    end_idx = len(text)
                    if end_markers:
                        for end_marker in end_markers:
                            temp_end = text.upper().find(end_marker.upper(), start_idx)
                            if temp_end != -1 and temp_end < end_idx:
                                end_idx = temp_end
                    
                    section = text[start_idx:end_idx].strip()
                    section = section.replace("##", "").replace("#", "").strip()
                    
                    if len(section) > 10:
                        return section
            except:
                continue
        return ""
    
    def _extract_robust_list(self, text: str, start_markers: List[str], end_markers: List[str]) -> List[str]:
        """Extra√ß√£o robusta de listas"""
        section = self._extract_robust_section(text, start_markers, end_markers)
        if not section:
            return []
        
        items = []
        lines = section.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith(('-', '‚Ä¢', '*', '1.', '2.', '3.', '4.', '5.')):
                item = line.lstrip('-‚Ä¢*123456789. ').strip()
                if len(item) > 15:
                    items.append(item)
        
        return items[:5]
    
    def _generate_fallback_domain_analysis(self, df: pd.DataFrame) -> str:
        """Gera an√°lise de dom√≠nio robusta"""
        cols_lower = [col.lower() for col in df.columns]
        
        if any(word in ' '.join(cols_lower) for word in ['transaction', 'amount', 'fraud', 'class']):
            return "Dataset de Detec√ß√£o de Fraude - baseado em colunas de transa√ß√£o e classifica√ß√£o"
        elif any(word in ' '.join(cols_lower) for word in ['price', 'sales', 'revenue', 'customer']):
            return "Dataset de Vendas/E-commerce - baseado em vari√°veis comerciais"
        elif any(word in ' '.join(cols_lower) for word in ['time', 'timestamp', 'date']):
            return "Dataset Temporal - cont√©m componentes de s√©rie temporal"
        elif len(df.select_dtypes(include=[np.number]).columns) > len(df.columns) * 0.7:
            return "Dataset Quantitativo - predominantemente num√©rico para an√°lise estat√≠stica"
        else:
            return f"Dataset Misto - {len(df.select_dtypes(include=[np.number]).columns)} vari√°veis num√©ricas e {len(df.select_dtypes(include=['object']).columns)} categ√≥ricas"
    
    def _generate_fallback_quality_analysis(self, df: pd.DataFrame) -> str:
        """Gera an√°lise de qualidade robusta"""
        completeness = ((df.shape[0] * df.shape[1] - df.isnull().sum().sum()) / (df.shape[0] * df.shape[1]) * 100)
        duplicates = df.duplicated().sum()
        
        if completeness > 95 and duplicates == 0:
            quality_score = "EXCELENTE"
        elif completeness > 90:
            quality_score = "BOA"
        elif completeness > 80:
            quality_score = "MODERADA"
        else:
            quality_score = "BAIXA"
        
        return f"Qualidade {quality_score}: {completeness:.1f}% completo, {duplicates:,} duplicatas, {df.shape[0]:,} registros v√°lidos para an√°lise"
    
    def _generate_fallback_characteristics(self, df: pd.DataFrame) -> List[str]:
        """Gera caracter√≠sticas robustas"""
        characteristics = []
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            skew_analysis = df[numeric_cols].skew()
            highly_skewed = sum(abs(skew_analysis) > 1)
            characteristics.append(f"Distribui√ß√µes: {highly_skewed} de {len(numeric_cols)} vari√°veis num√©ricas apresentam assimetria alta")
            
            try:
                cv_values = df[numeric_cols].std() / df[numeric_cols].mean()
                cv_mean = cv_values.mean()
                if not np.isnan(cv_mean) and np.isfinite(cv_mean):
                    if cv_mean > 1:
                        characteristics.append(f"Variabilidade alta: coeficiente de varia√ß√£o m√©dio de {cv_mean:.2f} indica dados heterog√™neos")
                    else:
                        characteristics.append(f"Variabilidade moderada: coeficiente de varia√ß√£o m√©dio de {cv_mean:.2f}")
                else:
                    characteristics.append("Variabilidade: n√£o calcul√°vel devido a valores zero ou inv√°lidos")
            except:
                characteristics.append("An√°lise de variabilidade: dados requerem pr√©-processamento")
            
            if len(numeric_cols) >= 2:
                try:
                    corr_matrix = df[numeric_cols].corr()
                    high_corr_count = 0
                    for i in range(len(corr_matrix.columns)):
                        for j in range(i+1, len(corr_matrix.columns)):
                            if abs(corr_matrix.iloc[i, j]) > 0.7 and not np.isnan(corr_matrix.iloc[i, j]):
                                high_corr_count += 1
                    characteristics.append(f"Estrutura de correla√ß√£o: {high_corr_count} pares de vari√°veis altamente correlacionadas")
                except:
                    characteristics.append("Estrutura de correla√ß√£o: an√°lise requer limpeza de dados")
        
        categorical_cols = df.select_dtypes(include=['object']).columns
        if len(categorical_cols) > 0:
            high_cardinality = sum(df[col].nunique() > 50 for col in categorical_cols)
            if high_cardinality > 0:
                characteristics.append(f"Cardinalidade: {high_cardinality} vari√°veis categ√≥ricas com alta diversidade (>50 valores √∫nicos)")
        
        if not characteristics:
            characteristics = ["Dataset com estrutura padr√£o para an√°lise explorat√≥ria"]
        
        return characteristics[:3]
    
    def _generate_fallback_recommendations(self, df: pd.DataFrame) -> List[str]:
        """Gera recomenda√ß√µes robustas"""
        recommendations = []
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) >= 2:
            recommendations.append(f"An√°lise de correla√ß√£o entre {len(numeric_cols)} vari√°veis num√©ricas para identificar relacionamentos lineares")
        
        if len(numeric_cols) > 0:
            recommendations.append("Detec√ß√£o de outliers multivariada usando Isolation Forest para identificar anomalias")
        
        if df.shape[0] > 1000:
            recommendations.append("Clustering hier√°rquico ou K-means para segmenta√ß√£o e identifica√ß√£o de padr√µes latentes")
        
        time_cols = [col for col in df.columns if 'time' in col.lower() or 'date' in col.lower()]
        if time_cols:
            recommendations.append(f"An√°lise de s√©ries temporais na coluna {time_cols[0]} para identificar tend√™ncias e sazonalidade")
        
        if not recommendations:
            recommendations = ["An√°lise explorat√≥ria sistem√°tica das vari√°veis principais"]
        
        return recommendations[:3]
    
    def _generate_fallback_insights(self, df: pd.DataFrame) -> List[str]:
        """Gera insights potenciais robustos"""
        insights = []
        
        if df.shape[0] > 10000:
            insights.append(f"Grande volume de dados ({df.shape[0]:,} registros) permite an√°lises estat√≠sticas robustas e modelagem preditiva")
        
        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols[:2]:
            value_counts = df[col].value_counts()
            if len(value_counts) == 2:
                balance_ratio = value_counts.min() / value_counts.max()
                if balance_ratio < 0.1:
                    insights.append(f"Forte desbalanceamento na vari√°vel {col} ({balance_ratio:.2f}) sugere necessidade de t√©cnicas de balanceamento")
        
        missing_cols = [col for col in df.columns if df[col].isnull().sum() > 0]
        if len(missing_cols) > df.shape[1] * 0.3:
            insights.append(f"Padr√£o de dados faltantes em {len(missing_cols)} vari√°veis pode indicar processo de coleta estruturado")
        
        if not insights:
            insights = ["Potencial para descoberta de padr√µes n√£o √≥bvios atrav√©s de an√°lise multivariada"]
        
        return insights[:3]

    def _add_to_gemini_memory(self, analysis_type: str, data: Dict):
        """Adiciona an√°lise √† mem√≥ria do agente Gemini."""
        if "gemini_memory" not in st.session_state:
            st.session_state.gemini_memory = []
        
        st.session_state.gemini_memory.append({
            "type": analysis_type,
            "data": data,
            "timestamp": datetime.now()
        })

    def get_full_memory_summary(self) -> str:
        """Gera um resumo completo de todas as intera√ß√µes e an√°lises na mem√≥ria do Gemini."""
        summary_text = """
ü§ñ **RESUMO COMPLETO DAS AN√ÅLISES - AGENTE AUT√îNOMO COM GEMINI**

"""
        if not hasattr(st.session_state, 'gemini_memory') or not st.session_state.gemini_memory:
            return summary_text + "Nenhuma an√°lise realizada ainda."

        for i, entry in enumerate(st.session_state.gemini_memory):
            try:
                summary_text += f"\n--- **An√°lise {i+1}: {entry['type'].replace('_', ' ').title()}** ---\n"
                summary_text += f"**Timestamp:** {entry['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\n"
                
                if entry["type"] == "initial_analysis":
                    full_response = entry["data"].get("full_response", "An√°lise n√£o dispon√≠vel")
                    summary_text += f"**An√°lise Inicial do Dataset:**\n{full_response}\n"
                elif entry["type"] == "user_query":
                    query = entry["data"].get("query", "Pergunta n√£o dispon√≠vel")
                    plan = entry["data"].get("plan", {})
                    results = entry["data"].get("results", {})
                    response = entry["data"].get("response", "Resposta n√£o dispon√≠vel")
                    
                    summary_text += f"**Pergunta do Usu√°rio:** {query}\n"
                    summary_text += f"**Plano de An√°lise:** {json.dumps(plan, indent=2)}\n"
                    
                    text_output = results.get("text_output", "Resultados n√£o dispon√≠veis")
                    summary_text += f"**Resultados da Execu√ß√£o:**\n```\n{text_output}\n```\n"
                    summary_text += f"**Resposta Final do Gemini:**\n{response}\n"
            except Exception as e:
                summary_text += f"**Erro ao processar an√°lise {i+1}:** {str(e)}\n"
            
        return summary_text

    def generate_smart_suggestions(self, df: pd.DataFrame) -> List[str]:
        """Gera sugest√µes inteligentes baseadas no dataset usando LLM"""
        
        system_context = """Voc√™ √© um especialista em an√°lise de dados. Gere 5-6 sugest√µes espec√≠ficas e pr√°ticas de perguntas que o usu√°rio pode fazer sobre seus dados."""
        
        user_prompt = f"""
        DATASET CONTEXT:
        - Tipo: {self.dataset_context.get("dataset_type", "Gen√©rico")}
        - Shape: {df.shape[0]:,} linhas x {df.shape[1]} colunas
        - Colunas num√©ricas: {list(df.select_dtypes(include=[np.number]).columns)[:8]}
        - Colunas categ√≥ricas: {list(df.select_dtypes(include=["object"]).columns)[:8]}
        
        AN√ÅLISES J√Å REALIZADAS:
        {len(self.conversation_history)} intera√ß√µes anteriores
        
        Gere 5-6 sugest√µes espec√≠ficas de perguntas que seriam valiosas para este dataset.
        Formato: "Pergunta espec√≠fica e clara"
        
        Exemplo de formato:
        - "Quais s√£o as correla√ß√µes mais fortes entre as vari√°veis num√©ricas?"
        - "Existem outliers significativos na coluna Amount?"
        """
        
        response = self._call_gemini(user_prompt, system_context)
        
        suggestions = []
        for line in response.split("\n"):
            line = line.strip()
            if line.startswith("-") or line.startswith("‚Ä¢") or line.startswith("*"):
                suggestion = line[1:].strip().strip("\"")
                if suggestion:
                    suggestions.append(suggestion)
        
        return suggestions[:6]

# Fun√ß√µes auxiliares para compatibilidade
def get_dataset_info(df):
    """Retorna informa√ß√µes descritivas completas do dataset."""
    info = f"""
üìä **INFORMA√á√ïES DESCRITIVAS DO DATASET**

**Dimens√µes:**
- Linhas: {df.shape[0]:,}
- Colunas: {df.shape[1]}
- Tamanho em mem√≥ria: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB

**Qualidade dos Dados:**
- Valores nulos: {df.isnull().sum().sum():,}
- Valores √∫nicos (m√©dia): {df.nunique().mean():.0f}
- Completude: {((df.shape[0] * df.shape[1] - df.isnull().sum().sum()) / (df.shape[0] * df.shape[1]) * 100):.1f}%

**Tipos de Colunas:**
- Num√©ricas: {len(df.select_dtypes(include=[np.number]).columns)}
- Categ√≥ricas: {len(df.select_dtypes(include=["object", "category"]).columns)}
- Datetime: {len(df.select_dtypes(include=["datetime"]).columns)}

**Estat√≠sticas Gerais:**
- Densidade de dados: {(df.count().sum() / (df.shape[0] * df.shape[1]) * 100):.1f}%
- Variabilidade m√©dia: {df.select_dtypes(include=[np.number]).std().mean():.2f}
"""
    return info

def generate_pdf_report(df, agent: GeminiAgent):
    """Gera relat√≥rio PDF com todas as an√°lises e informa√ß√µes do dataset."""
    if not PDF_AVAILABLE:
        raise ImportError("PdfPages n√£o est√° dispon√≠vel. Instale matplotlib com suporte a PDF.")
    
    pdf_buffer = io.BytesIO()
    
    try:
        with PdfPages(pdf_buffer) as pdf:
            # P√°gina 1: Capa e informa√ß√µes do dataset
            fig = plt.figure(figsize=(8.27, 11.69), dpi=100)
            ax = fig.add_subplot(111)
            
            ax.text(0.5, 0.95, "ü§ñ RELAT√ìRIO COMPLETO DE AN√ÅLISE DE DADOS", 
                    ha="center", va="top", fontsize=16, fontweight="bold")
            ax.text(0.5, 0.90, "Agente Aut√¥nomo com IA Generativa", 
                    ha="center", va="top", fontsize=12)
            ax.text(0.5, 0.87, f"Gerado em: {datetime.now().strftime('%d/%m/%Y %H:%M')}", 
                    ha="center", va="top", fontsize=10)
            
            dataset_info = get_dataset_info(df)
            ax.text(0.05, 0.80, dataset_info, ha="left", va="top", fontsize=8, 
                    wrap=True, bbox=dict(boxstyle="round,pad=0.3", facecolor="#e0f7fa", edgecolor="#00bcd4"))
            
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis("off")
            pdf.savefig(fig, bbox_inches="tight")
            plt.close(fig)

            # P√°ginas subsequentes: An√°lises realizadas
            if hasattr(st.session_state, 'gemini_memory') and st.session_state.gemini_memory:
                for i, entry in enumerate(st.session_state.gemini_memory):
                    try:
                        fig = plt.figure(figsize=(8.27, 11.69), dpi=100)
                        ax = fig.add_subplot(111)
                        
                        title = f"An√°lise {i+1}: {entry['type'].replace('_', ' ').title()}"
                        ax.text(0.05, 0.95, title, ha="left", va="top", fontsize=14, fontweight="bold")
                        
                        content = ""
                        if entry["type"] == "initial_analysis":
                            data = entry.get("data", {})
                            content = data.get("full_response", "An√°lise inicial n√£o dispon√≠vel")
                        elif entry["type"] == "user_query":
                            data = entry.get("data", {})
                            query = data.get("query", "Pergunta n√£o dispon√≠vel")
                            response = data.get("response", "Resposta n√£o dispon√≠vel")
                            
                            content = f"Pergunta: {query}\n\nResposta Final:\n{response}"
                        
                        content = content[:1500] if content else "Conte√∫do n√£o dispon√≠vel"
                        
                        ax.text(0.05, 0.90, content, ha="left", va="top", fontsize=8)
                        ax.set_xlim(0, 1)
                        ax.set_ylim(0, 1)
                        ax.axis("off")
                        pdf.savefig(fig, bbox_inches="tight")
                        plt.close(fig)

                        # Adicionar gr√°ficos se existirem
                        if (entry["type"] == "user_query" and 
                            entry.get("data", {}).get("visualization") and 
                            entry["data"]["visualization"].get("status") == "success" and
                            entry["data"]["visualization"].get("figure")):
                            
                            try:
                                fig_vis = entry["data"]["visualization"]["figure"]
                                if fig_vis:
                                    pdf.savefig(fig_vis, bbox_inches="tight")
                                    plt.close(fig_vis)
                            except Exception:
                                continue
                                
                    except Exception as e:
                        fig = plt.figure(figsize=(8.27, 11.69), dpi=100)
                        ax = fig.add_subplot(111)
                        ax.text(0.05, 0.95, f"Erro na An√°lise {i+1}", ha="left", va="top", fontsize=14, fontweight="bold")
                        ax.text(0.05, 0.90, f"Erro: {str(e)}", ha="left", va="top", fontsize=10)
                        ax.set_xlim(0, 1)
                        ax.set_ylim(0, 1)
                        ax.axis("off")
                        pdf.savefig(fig, bbox_inches="tight")
                        plt.close(fig)
            else:
                fig = plt.figure(figsize=(8.27, 11.69), dpi=100)
                ax = fig.add_subplot(111)
                ax.text(0.5, 0.5, "Nenhuma an√°lise realizada ainda.\nFa√ßa algumas perguntas ao agente primeiro.", 
                       ha="center", va="center", fontsize=12)
                ax.set_xlim(0, 1)
                ax.set_ylim(0, 1)
                ax.axis("off")
                pdf.savefig(fig, bbox_inches="tight")
                plt.close(fig)
        
        pdf_buffer.seek(0)
        return pdf_buffer.getvalue()
        
    except Exception as e:
        st.error(f"Erro ao gerar PDF: {str(e)}")
        return generate_text_report_fallback(df, agent)

def generate_text_report_fallback(df, agent: GeminiAgent):
    """Fallback: gera relat√≥rio em texto quando PDF falha"""
    try:
        report_content = agent.get_full_memory_summary()
        
        dataset_info = get_dataset_info(df)
        full_report = f"""
RELAT√ìRIO COMPLETO DE AN√ÅLISE DE DADOS
Gerado em: {datetime.now().strftime('%d/%m/%Y %H:%M')}

{dataset_info}

{report_content}
"""
        
        return full_report.encode('utf-8')
        
    except Exception as e:
        simple_report = f"""
RELAT√ìRIO DE AN√ÅLISE - ERRO NA GERA√á√ÉO
Gerado em: {datetime.now().strftime('%d/%m/%Y %H:%M')}

Dataset: {df.shape[0]} linhas x {df.shape[1]} colunas

Erro na gera√ß√£o do relat√≥rio: {str(e)}

Para obter um relat√≥rio completo, tente:
1. Verificar as depend√™ncias do matplotlib
2. Executar algumas an√°lises primeiro
3. Verificar a configura√ß√£o do Gemini
"""
        return simple_report.encode('utf-8')

def initialize_gemini_agent():
    """Inicializa o agente Gemini no Streamlit"""
    if 'gemini_agent' not in st.session_state:
        st.session_state.gemini_agent = GeminiAgent()
    
    return st.session_state.gemini_agent

def get_adaptive_suggestions(df):
    """Fun√ß√£o de compatibilidade - usa o agente Gemini se dispon√≠vel"""
    if 'gemini_agent' in st.session_state and st.session_state.gemini_agent.model:
        return st.session_state.gemini_agent.generate_smart_suggestions(df)
    else:
        return [
            "Mostre estat√≠sticas descritivas das colunas num√©ricas",
            "Analise correla√ß√µes entre as vari√°veis",
            "Detecte outliers nos dados",
            "Fa√ßa clustering autom√°tico",
            "Mostre a distribui√ß√£o da coluna principal",
            "Qual a mem√≥ria do agente?"
        ]

